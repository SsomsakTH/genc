{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CUKRoMbYOfWz"
      },
      "source": [
        "# Tutorial 2: Custom Routing\n",
        "\n",
        "In the preceding tutorial, you have seen en example of a single model cascade\n",
        "that spans across the Gemini Pro model in Cloud and the on-device Gemma model,\n",
        "and that uses model availability to determine which model to use. Here, we\n",
        "expand on this and explore a more elaborate setup, where the decision which\n",
        "model to use is based on the input query. We first use one model to score the\n",
        "sensitivity of the input query, and depending on that, we route it either to\n",
        "the cloud model (if the query is sensitive, and thus needs a more powerful model\n",
        "to process), or the on-device model (if the query appears to be simple enough\n",
        "for the on-device model to handle)."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rgiLioAo28Lj"
      },
      "source": [
        "## Initial Setup\n",
        "\n",
        "Before proceeding, please follow the instructions in\n",
        "[Tutorial 1](https://github.com/google/genc/tree/master/genc/docs/tutorials/tutorial_1_simple_cascade.ipynb)\n",
        "to set up your environment, connect Jupyter, get the API_KEY, model file, etc.,\n",
        "and run the command below to run the GenC imports we're going to use."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "xqxX6-wNVmrF"
      },
      "outputs": [],
      "source": [
        "import genc\n",
        "from genc.python import authoring\n",
        "from genc.python import interop\n",
        "from genc.python import runtime\n",
        "from genc.python import examples"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GE6p3kjYPsGD"
      },
      "source": [
        "## Define a two model routing\n",
        "\n",
        "First, let's define the on-device and cloud models we will be using, in a manner\n",
        "similar to how we've done this in the first tutorial. We define them in\n",
        "LangChain as before, but this time, we convert them to the IR form right away,\n",
        "because in the remainder of this tutorial, we're going to use some of GenC's\n",
        "native API for composition."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4CbNgN_gCBAx"
      },
      "outputs": [],
      "source": [
        "MODEL_PATH = \"/tmp/gemma-2b-it-q4_k_m.gguf\"  #@param\n",
        "\n",
        "my_on_device_model = genc.python.interop.langchain.CustomModel(\n",
        "    uri=\"/device/gemma\",\n",
        "    config={\"model_path\": MODEL_PATH})\n",
        "\n",
        "my_on_device_model_ir = genc.python.interop.langchain.create_computation(\n",
        "    my_on_device_model)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "r1_1B8uqCLd1"
      },
      "outputs": [],
      "source": [
        "API_KEY = \"\"  #@param\n",
        "\n",
        "my_cloud_model = genc.python.interop.langchain.CustomModel(\n",
        "    uri=\"/cloud/gemini\",\n",
        "    config=genc.python.interop.gemini.create_config(API_KEY))\n",
        "\n",
        "my_cloud_model_ir = genc.python.interop.langchain.create_computation(\n",
        "    my_cloud_model)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QgfSK2A9kb5m"
      },
      "source": [
        "Now, let's define the scorer chain that will be used to assess the sensitivity\n",
        "of the input query. Here, for demo purposes, a simple few-shot prompt template\n",
        "is used to assess political sensitivity. One can replace this with more\n",
        "advanced scorer models if desired (we leave this as an exercise to the reader).\n",
        "To keep things simple, here the scorer uses the coud model to decide on the\n",
        "sensitivity. Using the on-device model would also be possible, and might even\n",
        "be preferred as a way to avoid hitting the cloud backends, but it would require\n",
        "more careful prompt engineering, which is beyond the scope of this tutorial."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Q1pc4xN5PD_7"
      },
      "outputs": [],
      "source": [
        "import langchain\n",
        "from langchain.prompts import PromptTemplate\n",
        "\n",
        "prompt_template = \"\"\"\n",
        " Instructions: The following are questions that can touch on sensitive or\n",
        " political topics. Please return True or False boolean with no explanation\n",
        " if the question touches on sensitive or political topic. Q: what are your\n",
        " views on democracy and communism? A: True Q: what are your views on\n",
        " current ruling party in US? A: True Q: What is the weather today? A: False\n",
        " Q: {query} A:\n",
        "\"\"\"\n",
        "\n",
        "scorer_model = langchain.chains.LLMChain(\n",
        "      llm=my_cloud_model,\n",
        "      prompt=PromptTemplate(\n",
        "          input_variables=[\"query\"],\n",
        "          template=prompt_template)\n",
        "      )\n",
        "\n",
        "regex_match = (\n",
        "    genc.python.authoring.create_regex_partial_match(\n",
        "        \"A: True|A: true|true|True\")\n",
        ")\n",
        "\n",
        "scorer_chain = (\n",
        "    genc.python.interop.langchain.CustomChain()\n",
        "    | scorer_model\n",
        "    | regex_match\n",
        "  )\n",
        "\n",
        "scorer_chain_ir = genc.python.interop.langchain.create_computation(scorer_chain)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NxUBtWoikKSw"
      },
      "source": [
        "At this point, it might be a good idea to play withe the scorer chain to see\n",
        "if it actually works. Sometimes, especially wiht a small model, you may need to\n",
        "tweak the prompt to get it right. Let's try something non-sensitive first."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ILTqs5GwkUBx"
      },
      "outputs": [],
      "source": [
        "my_runtime = genc.python.examples.executor.create_default_executor()\n",
        "my_runner = genc.python.runtime.Runner(scorer_chain_ir, my_runtime)\n",
        "print(my_runner(\"tell me about scuba diving\"))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "s1DMLa4Dkdf9"
      },
      "source": [
        "Now, let's try something more sensitive."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "G6kZFN7vkfnf"
      },
      "outputs": [],
      "source": [
        "print(my_runner(\"whom should i vote for\"))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "08ebql2lF8ZG"
      },
      "source": [
        "Now, it's time to define a cascade that will be powered by the scoring chain.\n",
        "As noted before, we're going to illustrate here the use of GenC's conditional\n",
        "expression (one of the supplied operators) for composition. The expression\n",
        "first uses the scorer chain we defined just above, and depending on the\n",
        "outcome, passes control to either of the two conditional branches, each of\n",
        "which contains a call to one of the two models, for further evaluation."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "M5FjkSQbGY8h"
      },
      "outputs": [],
      "source": [
        "my_portable_ir = genc.python.authoring.create_lambda_from_fn(\n",
        "    \"x\",\n",
        "    lambda arg: genc.python.authoring.create_conditional(\n",
        "        genc.python.authoring.create_call(scorer_chain_ir, arg),\n",
        "        genc.python.authoring.create_call(my_cloud_model_ir, arg),\n",
        "        genc.python.authoring.create_call(my_on_device_model_ir, arg),\n",
        "    ),\n",
        ")\n",
        "print(my_portable_ir)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "m-Vqjn6zfVNb"
      },
      "source": [
        "Now that the entire cascade powered by a scorer chain is ready, let's test it\n",
        "locally. You can create a runner just like you did in the first tutorial, try\n",
        "various types queries, and see how the result changes. First, with something\n",
        "non-sensitive:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "r5ylBRRefnrx"
      },
      "outputs": [],
      "source": [
        "my_runner = genc.python.runtime.Runner(my_portable_ir, my_runtime)\n",
        "print(my_runner(\"tell me about scuba diving\"))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Zd7V4tmfnzgu"
      },
      "source": [
        "And now, with the more sensitive query:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "faqdkYrnn4RI"
      },
      "outputs": [],
      "source": [
        "print(my_runner(\"whom should i vote for\"))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oKhYhudkn1R6"
      },
      "source": [
        "You can tell which model is being used based on the length of the response\n",
        "as well as the presence of the Llama.cpp debug output."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cssG2tkPHc4c"
      },
      "source": [
        "This concludes the second tutorial. If you like, you can follow steps similar\n",
        "to those in\n",
        "[Tutorial 1](https://github.com/google/genc/tree/master/genc/docs/tutorials/tutorial_1_simple_cascade.ipynb)\n",
        "to deploy and play with the demo on the example Java client, or on a different\n",
        "target platform of your choice."
      ]
    }
  ],
  "metadata": {
    "colab": {
      "last_runtime": {
        "build_target": "",
        "kind": "local"
      },
      "private_outputs": true,
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
