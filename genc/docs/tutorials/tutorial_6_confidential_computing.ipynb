{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gKlYiqIQSau_"
      },
      "source": [
        "# Tutorial 6: Confidential Computing\n",
        "\n",
        "In this tutorial, we set up a GenC-based service to run in a\n",
        "[Trusted Execution Environment (TEE)](https://en.wikipedia.org/wiki/Trusted_execution_environment)\n",
        "using the\n",
        "[Confidential Computing](https://cloud.google.com/security/products/confidential-computing)\n",
        "capabilities on the\n",
        "[Google Cloud Platform (GCP)](https://cloud.google.com/)\n",
        "to offload your GenAI workloads from a remote client to securely run in the\n",
        "Cloud, with formal cryptographic assurances that your data and results will\n",
        "remain confidential.\n",
        "\n",
        "## Overview\n",
        "\n",
        "The overall architecture of the example we're going to build up to is as shown\n",
        "on the diagram below.\n",
        "\n",
        "![GenC in a TEE](genc_tee.png)\n",
        "\n",
        "Let's go through this diagram step-by-step:\n",
        "\n",
        "1.   We use the\n",
        "     [Confidential Space](https://cloud.google.com/docs/security/confidential-space)\n",
        "     service on GCP to setup a\n",
        "     [Confidential VM](https://cloud.google.com/confidential-computing/confidential-vm/docs/confidential-vm-overview),\n",
        "     a virtual machine running on specialized hardware that supports trusted\n",
        "     execution, with a Confidential Space system image that provides\n",
        "     additional services such as generating\n",
        "     [attestation](https://cloud.google.com/confidential-computing/confidential-vm/docs/attestation)\n",
        "     reports.\n",
        "\n",
        "2.   Within the\n",
        "     [Trusted Execution Environment (TEE)](https://en.wikipedia.org/wiki/Trusted_execution_environment)\n",
        "     created in that VM, we deploy a GenC service\n",
        "     [container](https://docs.docker.com/reference/cli/docker/container/)\n",
        "     based on our\n",
        "     [Dockerfile](../../cc/examples/confidential_computing/Dockerfile).\n",
        "     The container is setup similarly to the one you used for development,\n",
        "     with an Ubuntu image a copy of GenC from GitHub, but unlike the one used\n",
        "     for development, instead of an interactive prompt, it's set to launch a\n",
        "     [small C++ binary](../../cc/examples/worker/server.cc) that listens for\n",
        "     incoming HTTP connections on port 80, and hosts a GenC runtime configured\n",
        "     with the\n",
        "     [Gemma 2B model](https://huggingface.co/google/gemma-2b),\n",
        "     the same as what we used in other tutorials.\n",
        "\n",
        "3.   A remote client (here, this Colab notebook process, or an app on a mobile\n",
        "     device when we later deploy the code on a phone) submits IR to its local\n",
        "     GenC runtime that may contain a chunk of processing to be performed in the\n",
        "     Cloud, in a confidential manner. The GenC runtime on the client initiates\n",
        "     a HTTP connection to the server mentioned above, and establishes a\n",
        "     communication channel with the GenC runtime that runs privately in a TEE.\n",
        "     In this tutorial, we use a plain HTTP connection, but you can change this\n",
        "     to use HTTPS (see [this example](../../cc/examples/worker/README.md) for\n",
        "     details on how to do this).\n",
        "\n",
        "4.   The first request the remote client issues to the server in a TEE is to\n",
        "     fetch the\n",
        "     [attestation](https://cloud.google.com/confidential-computing/confidential-vm/docs/attestation)\n",
        "     report, which includes a public key that will be used to eatablish an\n",
        "     encrypted communication channel and information about the software that\n",
        "     runs on the server (including the SHA256 digest of the deployed image),\n",
        "     signed by the trusted computing platform. The GenC binary on the server\n",
        "     uses crypto libraries from\n",
        "     [Project Oak](https://github.com/project-oak/oak)\n",
        "     to generate the keys, and the attestation API provided by Confidential\n",
        "     Space via a local UNIX socket in the TEE to obtain the attestation report\n",
        "     in the form of a [JSON Web Token (JWT)](https://jwt.io/introduction) that\n",
        "     contains a number of\n",
        "     [claims](https://cloud.google.com/confidential-computing/confidential-space/docs/reference/token-claims),\n",
        "     including those mentioned above (the signed copy of the public key to use\n",
        "     for encrypted communication, the SHA256 digest of the docker container\n",
        "     image, etc.). This is relayed back to the client.\n",
        "\n",
        "5.   The client verifies the validity of the attestation report to confirm that\n",
        "     it's interacting with a service that runs the code expected by the client.\n",
        "     In particular, the client verifies that the image digest matches the one\n",
        "     that was obtained by the developer when they built and pushed the GenC\n",
        "     image to run on GCP (the developer may distribute the image digest along\n",
        "     with their app, or as in this example, the developer and the user running\n",
        "     the app may be the same person; many other arrangements are possible, and\n",
        "     will be discussed in future tutorials).\n",
        "\n",
        "6.   Upon confirming that it's talking to a server built with the OSS code from\n",
        "     the GenC repo distributed as a part of this tutorial, the client proceeds\n",
        "     to establish an encrypted communication channel, through which it submits\n",
        "     a remote execution request (in the form of GenC IR representing a remote\n",
        "     computation, and serialized arguments to feed as input), and retrieves the\n",
        "     results. The client talks to the server using an encrypted gRPC\n",
        "     incarnation of the same general-purpose\n",
        "     [`Executor` interface](https://github.com/google/genc/blob/master/genc/docs/runtime.md)\n",
        "     that we use in all other examples. The interface supports stateful,\n",
        "     multi-round communication, albeit in this particular example, we only\n",
        "     issue one-off requests, and the server is not configured with any\n",
        "     persistent memory that would enable it to host user data or persist state.\n",
        "\n",
        "Now that you understand the basic flow, let's try it out.\n",
        "\n",
        "## Step 0. Configure your client environment.\n",
        "\n",
        "As in all other tutorials, you need to connect this Python colab notebook to a\n",
        "Jupyter runtime that can load GenC, so that you can execute the code shown\n",
        "below. Make sure to follow the basic steps in\n",
        "[SETUP.md](https://github.com/google/genc/tree/master/SETUP.md)\n",
        "at the root of the repo, and open this tutorial from the page served by the\n",
        "Jupyter process you launched as per the above, then execute the code below to\n",
        "confirm that your client setup works correctly."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "fR6MAVXlSaAg"
      },
      "outputs": [],
      "source": [
        "import genc\n",
        "from genc.python import authoring\n",
        "from genc.python import runtime"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DPJvHT68r7mQ"
      },
      "source": [
        "## Step 1. Create your own confidential computation service on GCP\n",
        "\n",
        "Next, we'll need to setup a confidential service on GCP that will handle your\n",
        "workloads.\n",
        "\n",
        "Start by following the usual steps to\n",
        "[create a new GCP project](https://cloud.google.com/resource-manager/docs/creating-managing-projects),\n",
        "and make sure that that firewall rules in your organization/project will allow\n",
        "inbound HTTP traffic (this will often be inherited from the organization where\n",
        "you're creating your project). Make sure that you have the\n",
        "[`gcloud`](https://cloud.google.com/sdk/docs/install) command-line tool setup,\n",
        "since we'll use it to automate the remaining steps.\n",
        "\n",
        "Once that's done, enter the\n",
        "[`cc/examples/confidential_computing`](../../cc/examples/confidential_computing)\n",
        "directory, and edit the\n",
        "[config_environment.sh](../../cc/examples/confidential_computing/config_environment.sh)\n",
        "script in that directory with your GCP project id, preferred VM and service\n",
        "account names, etc., and then execute the\n",
        "[create_environment.sh](../../cc/examples/confidential_computing/create_environment.sh)\n",
        "one-time setup script to populate the appropriate parts of your project (the\n",
        "service account, repository for images, basic permissions, etc.). Feel free to\n",
        "further tweak the setup if your organization has additional constraints. The\n",
        "goal is to be able to create new Confidential VMs with images you will build\n",
        "locally on your workstation and manually push to the Cloud image repo. You only\n",
        "need to run this setup once.\n",
        "```\n",
        "cd cc/examples/confidential_computing\n",
        "bash ./create_environment.sh\n",
        "```\n",
        "\n",
        "Once the above is done, you're ready to build and push the container image, as\n",
        "follows:\n",
        "\n",
        "```\n",
        "bash ./build_image.sh\n",
        "bash ./push_image.sh\n",
        "```\n",
        "\n",
        "After the above completes, the script will print text that looks like below:\n",
        "```\n",
        "latest: digest: sha256:SOMETHING size: WHATEVER\n",
        "```\n",
        "\n",
        "Copy the `sha256:SOMETHING` and plug it in below, then run the colab cell to\n",
        "retain the image digest. We're going to need this to setup the client to only\n",
        "forward requests to the image we deployed."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8Oa5jNy8wAAY"
      },
      "outputs": [],
      "source": [
        "image_digest = \"\" # copy here the `sha256:SOMETHING` part printed out by ./push_image.sh, then execute his call"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eHbTWg-8wIn5"
      },
      "source": [
        "Once the image was pushed (or sometimes, a minute later when things settle in),\n",
        "you're ready to deploy a new confidential VM that runs this image. In this\n",
        "example, we're going to use a `DEBUG` image for convenience, as that will enable\n",
        "you to see debug printouts on the serial console, and even SSH into the VM to\n",
        "debug it. For a real deployment (where you'll be sending actual confidential\n",
        "data), you'll want to disable the `DEBUG` flag to harden the image (so that no\n",
        "debug logs are printed, and noone can SSH into the VM, such that the only way\n",
        "in is through the HTTP endpoint we set up). For illustrate purposes, you can\n",
        "leave it as is.\n",
        "\n",
        "```\n",
        "bash ./create_debug_vm.sh\n",
        "```\n",
        "\n",
        "Once the above completes, you will see a printout showing the IP address of the\n",
        "server (under `EXTERNAL IP`). Copy it, enter below, and execute the following\n",
        "cell, so that we can use it later to setup the client."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Z7oghV_Vxldq"
      },
      "outputs": [],
      "source": [
        "server_ip = \"\" # copy here the `EXTERNAL IP` address of the server printed out by ./create_debug_vm.sh\n",
        "server_address =  server_ip + \":80\""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_JAX5T17xzSn"
      },
      "source": [
        "It often takes a few minutes for the Confidential VM to become ready to accept\n",
        "incoming requests. Since you're running in `DEBUG` mode, you can click through\n",
        "to the `VM instances` in your GCP dashboard, open your VM (it will be `worker`\n",
        "unless you renamed it), and follow to `serial port 1 (console)` under `Logs` to\n",
        "verify that the message `workload task started` appears at the end of the log\n",
        "stream. Later, when you disable the `DEBUG` mode, you may just want to wait for\n",
        "a minute or two.\n",
        "\n",
        "Once that's done, we're ready to launch the client side and submit a workload\n",
        "for confidential execution.\n",
        "\n",
        "## Step 2. Create and run a confidential computation\n",
        "\n",
        "The first step looks just like in [Tutorial 1](tutorial_1_simple_cascade.ipynb).\n",
        "We setup a simple chain that consists of a prompt and an LLM call. The\n",
        "confidential runtime image we deployed in the Cloud is configured with the\n",
        "[Gemma 2B model](https://huggingface.co/google/gemma-2b)\n",
        "model, bound to `/device/gemma`, just like in all other tutorials, for your\n",
        "convenience (so you can later play with redirecting code from other tutorials\n",
        "to run in the TEE).\n",
        "\n",
        "Let's go ahead and define the confidential workload:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8eaCG3O60Lby"
      },
      "outputs": [],
      "source": [
        "# Computation to be executed in a secure enclave on Confidential Computing.\n",
        "@genc.python.authoring.traced_computation\n",
        "def private_workload(x):\n",
        "  prompt_template = genc.python.authoring.prompt_template[\"Tell me about {topic}\"]\n",
        "  model_inference = genc.python.authoring.model_inference_with_config[{\n",
        "      \"model_uri\": \"/device/gemma\",\n",
        "      \"model_config\": {\"model_path\": \"/gemma-2b-it-q4_k_m.gguf\"}}]\n",
        "  return model_inference(prompt_template(x))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8AARJ8lv0WVj"
      },
      "source": [
        "Now, let's write another computation that will submit this workload to run in\n",
        "the confidential runtime you deployed on Cloud. You achieve this by using the\n",
        "`confidential computation` operator. The operator is parameterized with the\n",
        "computation being delegated, and parameters that describe the server where it\n",
        "should be forwarded, including the address and image digest you saved earlier."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ZPX_0DaG08TS"
      },
      "outputs": [],
      "source": [
        "# Computation to run locally, including the delegation of `foo` to the TEE.\n",
        "@genc.python.authoring.traced_computation\n",
        "def run_private_workload_on_my_trusted_server(x):\n",
        "  backend = {\"server_address\": server_address, \"image_digest\": image_digest}\n",
        "  return genc.python.authoring.confidential_computation[private_workload, backend](x)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rJ1-6Py71RuO"
      },
      "source": [
        "This is it! You can now run the above, just like you ran all other examples."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Q_meW71o1qAX"
      },
      "outputs": [],
      "source": [
        "genc.python.runtime.set_default_executor()\n",
        "result = run_private_workload_on_my_trusted_server(\"scuba diving\")\n",
        "print(result)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aGdzUG3n12MC"
      },
      "source": [
        "## Step 3. Deploy the client on a mobile device\n",
        "\n",
        "Coming soon..."
      ]
    }
  ],
  "metadata": {
    "colab": {
      "private_outputs": true,
      "provenance": []
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
