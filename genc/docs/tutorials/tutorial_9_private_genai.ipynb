{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nic31KHZClnv"
      },
      "source": [
        "# Tutorial 9: Private GenAI\n",
        "\n",
        "## Introduction and problem statement\n",
        "\n",
        "This tutorial synthesizes and combines concepts introduced in several of the\n",
        "preceding tutorials to support scenarios that focus on processing data that's\n",
        "private and/or confidential.\n",
        "\n",
        "The specific scenario we're going to focus on here is a mobile app on Android\n",
        "that wants to perform GenAI processing, where some of the data fed by the\n",
        "app as input to that processing might be sensitive.\n",
        "\n",
        "Now, whereas privacy is a multi-faceted concept, in this tutorial we're going\n",
        "to focus on one specific aspect of it: here, the app developer would like to\n",
        "(or has an obligation to) ensure that while the genAI processing takes place, access to the sensitive data used in this processing, or any intermediate or\n",
        "final results derived from it, remains tightly controlled.\n",
        "\n",
        "In particular, the developer seeks to avoid issuing LLM queries or any other\n",
        "service calls containing sensitive data to untrusted cloud backends, where\n",
        "the data might be at the risk of being collected/retained, and potentially\n",
        "later used for purposes that are incompatible with privacy or confidentiality\n",
        "expectations of the users or data owners, or that may violate the developer's\n",
        "obligations with respect to responsible handling of such data.\n",
        "\n",
        "## The approach we're going to follow in this tutorial\n",
        "\n",
        "* * *\n",
        "\n",
        "NOTE: Please be advised that GenC is positioned as a research and experimental\n",
        "framework. Use the techniques described here at your own risk. All code used\n",
        "here is built using off the shelf and open-source components; we encourage you\n",
        "to review the code to make an informed decision about potential suitability of\n",
        "this technology, architecture/design, or any of the ideas described here for\n",
        "non-research-oriented applications.\n",
        "\n",
        "* * *\n",
        "\n",
        "In the preceding tutorials, we've introduced a few mechanisms that a developer\n",
        "can use that we'll combine in this tutorial into a cohesive solution. The gist\n",
        "of the approach we'll take in this tutorial is to combine all these individual\n",
        "ingredients into a cohesive whole.\n",
        "\n",
        "The key ingredients:\n",
        "\n",
        "*   **Processing locally on-device**, e.g., on Android using the on-device LLM,\n",
        "    as covered in [Tutorial 8](tutorial_8_android.ipynb). During the process\n",
        "    of constructing the IR, the developer can simply avoid calls to cloud LLMs\n",
        "    or other external services. Albeit we won't go quite as far in this demo,\n",
        "    the developer could possibly go even further, if needed, and configure a\n",
        "    custom version of the GenC runtime for Android with support for calling any\n",
        "    external services removed, such that even if the IR might be created with\n",
        "    such calls in it (perhaps accidentally), they would fail to run.\n",
        "\n",
        "*   **Processing in a\n",
        "    [Trusted Execution Environment (TEE)](https://en.wikipedia.org/wiki/Trusted_execution_environment)**,\n",
        "    e.g., within a\n",
        "    [Confidential Computing](https://cloud.google.com/security/products/confidential-computing) VM instance setup on GCP by the developer. Here,\n",
        "    the developer relies on all data and processing being encrypted end-to-end\n",
        "    during transport and in memory, and on the formal and verifiable guarantees\n",
        "    offered by the specialized hardware that powers the TEE to ensure that\n",
        "    neither the cloud provider, nor the administrator/operator of the service\n",
        "    deployed in the TEE, can intercept the data or results. You've seen an\n",
        "    example of this in [Tutorial 6](tutorial_6_confidential_computing.ipynb).\n",
        "\n",
        "*   **Hybrid device and Cloud processing** that combines operations on-device an\n",
        "    in-Cloud, such that both on-device and Cloud components can work together\n",
        "    as a seamless whole to support the needs of the application. You've seen\n",
        "    elements of this in [Tutorial 1](tutorial_1_simple_cascase.ipynb) and\n",
        "    [Tutorial 2](tutorial_2_custom_routing.ipynb), where we combined on-device\n",
        "    and Cloud models into a device-to-Cloud model cascade, and demonstrated 2\n",
        "    forms of routing to decide which model to use for a particular query.\n",
        "    Whereas the examples shown in these 2 tutorials used an unprotected Cloud\n",
        "    model and weren't focusing on privacy, here the hybrid approach can enable\n",
        "    us to combine the on-device and TEE-based genAI processing just mentioned\n",
        "    above to form a private device-to-TEE model cascade that's protected from\n",
        "    intrusion by untrusted external parties.\n",
        "\n",
        "If you haven't reviewed yet the tutorials mentioned above, we'd like to strongly\n",
        "encourage you to do so, as doing that will make the rest of this tutorial much\n",
        "easier to follow (we don't explain some of the foundational concepts and APIs in\n",
        "quite the same depth as we did in the tutorials above).\n",
        "\n",
        "## Overall architecture\n",
        "\n",
        "The overall architecture used in this tutorial will be a combination of what\n",
        "you've seen in [Tutorial 6](tutorial_6_confidential_computing.ipynb) and\n",
        "[Tutorial 8](tutorial_8_android.ipynb), except with a larger model in the TEE,\n",
        "and with hybrid processing mixed in.\n",
        "\n",
        "See the following diagram:\n",
        "\n",
        "![Private GenAI](private_genai.png)\n",
        "\n",
        "Let's go through this diagram step-by-step:\n",
        "\n",
        "*   We will be using two LLMs, both of them instances of\n",
        "    [Gemma](https://ai.google.dev/gemma). One will be a small quantized 2B that\n",
        "    comes in at approximately 1.5GB package and fits on a modern Android\n",
        "    device (e.g., on a Pixel 7). Another will be a much larger, unquantized 7B\n",
        "    version that comes in slightly over 34GB, and doesn't fit on a mobile\n",
        "    device, but that can be hosted in a TEE (e.g., on any of the upper end of\n",
        "    the\n",
        "    [N2D standard](https://cloud.google.com/compute/docs/general-purpose-machines#n2d-standard)\n",
        "    machine family, one of which we're going to use here to power this tutorial.\n",
        "    The two of them together will be used to power the GenAI workloads defined\n",
        "    in this tutorial.\n",
        "\n",
        "*   Both instances of Gemma will run in protected environments. One directly\n",
        "    bundled with the mobile app, the other in a TEE, with encrypted memory and\n",
        "    other strong assurances offered by the trusted computing environment. The\n",
        "    device and the TEE are connected with an encrypted communication channel to\n",
        "    form a whole. You can think of the TEE as effectively a logical extension\n",
        "    of the mobile device. The device confirms the identify of the workload\n",
        "    running in the TEE by obtaining and subsequently verifying an\n",
        "    [attestation](https://cloud.google.com/confidential-computing/confidential-vm/docs/attestation) report that contains a `SHA256` digest of\n",
        "    the image. The developer who builds the image locally, and knows what\n",
        "    the `SHA256` image digest should be, embeds the digest directly in the IR,\n",
        "    as an integral part of the GenAI workload.\n",
        "\n",
        "*   Interaction with the two Gemma instances is mediated by two instances of\n",
        "    GenC runtime, one on-device, and one in the TEE that talk to one-another\n",
        "    and jointly execute the GenAI workload you define. The on-device runtime\n",
        "    delegates processing to the runtime in the TEE in a manner defined by the\n",
        "    developer (you), directly based on the intent declared in the code.\n",
        "\n",
        "*   The developer's intent and the logic to execute, as always, are defined in\n",
        "    the form of portable\n",
        "    [Intermediate Representation (IR)](https://github.com/google/genc/tree/master/genc/docs/ir.md). We'll author the IR in the colab, then\n",
        "    deploy and run it on-device. GenC runtime instances use chunks of the\n",
        "    same IR to exchange the GenAI logic to be delegated from device to cloud.\n",
        "\n",
        "Keep in mind that in this particular deployment scenario, the protections do\n",
        "inherently rely on all the code involved (GenC runtime, the model, etc.) being\n",
        "open-source. Use of closed-source or untrusted code in this type of environment\n",
        "can be possible, but it is more complex, and falls outside the scope of this\n",
        "introductory tutorial.\n",
        "\n",
        "## Initial setup\n",
        "\n",
        "As usual, we start by ensuring that you have the development environment that\n",
        "can support the remainder of this tutorial.\n",
        "\n",
        "The best way to go about this is to review the setup steps in\n",
        "[Tutorial 6](tutorial_6_confidential_computing.ipynb) and\n",
        "[Tutorial 8](tutorial_8_android.ipynb).\n",
        "\n",
        "Rather than repeating the content of those tutorials, we'll limit ourselves\n",
        "here to a quick checklist. Please consult the above for the detailed steps.\n",
        "\n",
        "*   Follow the steps in\n",
        "    [SETUP.md](https://github.com/google/genc/tree/master/SETUP.md)\n",
        "    to download GenC from GitHub, build it, and run the tests in a docker\n",
        "    container. Make sure you can launch the Jupyter container, and then\n",
        "    connect to it and reopen this notebook in Jupyter, so that you can execute\n",
        "    all the Python code below.\n",
        "\n",
        "*   Setup for Android development (Developer mode on your Android device, USB\n",
        "    debugging, the `adb` tool on your development workstation, etc.), then\n",
        "    confirm you can build and install the `GenC Demo App` on your device, as\n",
        "    described in [Tutorial 8](tutorial_8_android.ipynb). We will continue to\n",
        "    use the same demo app to execute the GenAI logic authored in this tutorial.\n",
        "\n",
        "*   Setup a GCP project in which we can host a Confidential VM, as described\n",
        "    in [Tutorial 6](tutorial_6_confidential_computing.ipynb), install the\n",
        "    [`gcloud`](https://cloud.google.com/sdk/docs/install) command-line tool,\n",
        "    and then, as in [Tutorial 6](tutorial_6_confidential_computing.ipynb),\n",
        "    enter the\n",
        "    [`confidential_computing`](https://github.com/google/genc/tree/master/genc/cc/examples/confidential_computing)\n",
        "    directory, edit the\n",
        "    [config_environment.sh](https://github.com/google/genc/tree/master/genc/cc/examples/confidential_computing/config_environment.sh)\n",
        "    script there to match your GCP project, account names, etc.\n",
        "\n",
        "*   Obtain an instance of quantized Gemma 2B weights that you will use as an\n",
        "    on-device LLM. This ia a file named `gemma-2b-it-q4_k_m.gguf`, 1495245728 bytes in size. Push this file to your mobile device using the `adb` tool,\n",
        "    as discussed in [Tutorial 8](tutorial_8_android.ipynb).\n",
        "\n",
        "Assuming you have all the above in place, there are a couple additional steps\n",
        "needed to upgrade the service image that runs in the TEE to use the larger\n",
        "Gemma 7B model.\n",
        "\n",
        "First, obtain an instance of unquantized Gemma 7B weights, e.g.,\n",
        "[from HuggingFace](https://huggingface.co/google/gemma-7b). Obtaining the file\n",
        "will require filling a form online, thus we can't auto-download it for you.\n",
        "You want to have a file named `gemma-7b-it.gguf` that is 34158344288 bytes in\n",
        "size before continuing.\n",
        "\n",
        "Next, copy this file to the\n",
        "[`confidential_computing`](https://github.com/google/genc/tree/master/genc/cc/examples/confidential_computing)\n",
        "directory that's to build images in\n",
        "[Tutorial 6](tutorial_6_confidential_computing.ipynb), such that it sits\n",
        "there side by side with the\n",
        "[`Dockerfile`](https://github.com/google/genc/tree/master/genc/cc/examples/confidential_computing/Dockerfile).\n",
        "\n",
        "Then, replace this section of the `Dockerfile`:\n",
        "\n",
        "```\n",
        "RUN wget --directory-prefix=/ https://huggingface.co/lmstudio-ai/gemma-2b-it-GGUF/resolve/main/gemma-2b-it-q4_k_m.gguf\n",
        "```\n",
        "\n",
        "Remove the above, and instead insert the following:\n",
        "\n",
        "```\n",
        "COPY gemma-7b-it.gguf /gemma-7b-it.gguf\n",
        "```\n",
        "\n",
        "Now, you can build and push the image of the runtime that runs in the TEE, as\n",
        "described in [Tutorial 6](tutorial_6_confidential_computing.ipynb). The above\n",
        "change you made ensures that it's the larger Gemma 7B that gets bundled with\n",
        "the runtime. Go ahead and run `bash ./build_image.sh` and `bash ./push_image.sh`\n",
        "as described in [Tutorial 6](tutorial_6_confidential_computing.ipynb).\n",
        "\n",
        "Before you create VM by calling the `bash ./create_debug_vm.sh` script, you\n",
        "will want to tweak the VM parameters to grant more resources to accommodate\n",
        "the larger model, by editing the content of that script and inserting these\n",
        "two lines in place of the existing `machine_type` setting:\n",
        "\n",
        "```\n",
        "  --machine-type=\"n2d-standard-96\" \\\n",
        "  --boot-disk-size=\"100GB\" \\\n",
        "```\n",
        "\n",
        "With that, you should be able to run the script to create a debug VM, and since\n",
        "we're in debug mode, you can confirm it's up as discussed in\n",
        "[Tutorial 6](tutorial_6_confidential_computing.ipynb) by verifying that the VM\n",
        "has printed the `workload task started` at the end of the log in the serial\n",
        "console, and that there's no error message indicating that the VM has aborted.\n",
        "Keep in mind that due to the large image and file sizes involved, and you might\n",
        "see messages about the `systemd-journald.service` crashing and restarting. Wait\n",
        "until it resolves. This could take an hour, but it should eventually converge,\n",
        "and you should see the TEE instance coming up and reporting readiness as noted\n",
        "by the message quoted above.\n",
        "\n",
        "As long as there are no firewall rules in place that would prevent you from\n",
        "connecting to the VM from your device, you should be good to go.\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "S7v0PkvvEZGW"
      },
      "source": [
        "## TO BE CONTINUED..."
      ]
    }
  ],
  "metadata": {
    "colab": {
      "private_outputs": true,
      "provenance": []
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
