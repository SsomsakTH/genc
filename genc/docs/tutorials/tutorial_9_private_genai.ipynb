{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nic31KHZClnv"
      },
      "source": [
        "# Tutorial 9: Private GenAI\n",
        "\n",
        "## Introduction and problem statement\n",
        "\n",
        "This tutorial synthesizes and combines concepts introduced in several of the\n",
        "preceding tutorials to support scenarios that focus on processing data that's\n",
        "private and/or confidential.\n",
        "\n",
        "The specific scenario we're going to focus on here is a mobile app on Android\n",
        "that wants to perform GenAI processing, where some of the data fed by the\n",
        "app as input to that processing might be sensitive.\n",
        "\n",
        "Now, whereas privacy is a multi-faceted concept, in this tutorial we're going\n",
        "to focus on one specific aspect of it: here, the app developer would like to\n",
        "(or has an obligation to) ensure that while the genAI processing takes place, access to the sensitive data used in this processing, or any intermediate or\n",
        "final results derived from it, remains tightly controlled.\n",
        "\n",
        "In particular, the developer seeks to avoid issuing LLM queries or any other\n",
        "service calls containing sensitive data to untrusted cloud backends, where\n",
        "the data might be at the risk of being collected/retained, and potentially\n",
        "later used for purposes that are incompatible with privacy or confidentiality\n",
        "expectations of the users or data owners, or that may violate the developer's\n",
        "obligations with respect to responsible handling of such data.\n",
        "\n",
        "## The approach we're going to follow in this tutorial\n",
        "\n",
        "* * *\n",
        "\n",
        "NOTE: Please be advised that GenC is positioned as a research and experimental\n",
        "framework. Use the techniques described here at your own risk. All code used\n",
        "here is built using off the shelf and open-source components; we encourage you\n",
        "to review the code to make an informed decision about potential suitability of\n",
        "this technology, architecture/design, or any of the ideas described here for\n",
        "non-research-oriented applications.\n",
        "\n",
        "* * *\n",
        "\n",
        "In the preceding tutorials, we've introduced a few mechanisms that a developer\n",
        "can use that we'll combine in this tutorial into a cohesive solution. The gist\n",
        "of the approach we'll take in this tutorial is to combine all these individual\n",
        "ingredients into a cohesive whole.\n",
        "\n",
        "The key ingredients:\n",
        "\n",
        "*   **Processing locally on-device**, e.g., on Android using the on-device LLM,\n",
        "    as covered in [Tutorial 8](tutorial_8_android.ipynb). During the process\n",
        "    of constructing the IR, the developer can simply avoid calls to cloud LLMs\n",
        "    or other external services. Albeit we won't go quite as far in this demo,\n",
        "    the developer could possibly go even further, if needed, and configure a\n",
        "    custom version of the GenC runtime for Android with support for calling any\n",
        "    external services removed, such that even if the IR might be created with\n",
        "    such calls in it (perhaps accidentally), they would fail to run.\n",
        "\n",
        "*   **Processing in a\n",
        "    [Trusted Execution Environment (TEE)](https://en.wikipedia.org/wiki/Trusted_execution_environment)**,\n",
        "    e.g., within a\n",
        "    [Confidential Computing](https://cloud.google.com/security/products/confidential-computing) VM instance setup on GCP by the developer. Here,\n",
        "    the developer relies on all data and processing being encrypted end-to-end\n",
        "    during transport and in memory, and on the formal and verifiable guarantees\n",
        "    offered by the specialized hardware that powers the TEE to ensure that\n",
        "    neither the cloud provider, nor the administrator/operator of the service\n",
        "    deployed in the TEE, can intercept the data or results. You've seen an\n",
        "    example of this in [Tutorial 6](tutorial_6_confidential_computing.ipynb).\n",
        "\n",
        "*   **Hybrid device and Cloud processing** that combines operations on-device an\n",
        "    in-Cloud, such that both on-device and Cloud components can work together\n",
        "    as a seamless whole to support the needs of the application. You've seen\n",
        "    elements of this in [Tutorial 1](tutorial_1_simple_cascase.ipynb) and\n",
        "    [Tutorial 2](tutorial_2_custom_routing.ipynb), where we combined on-device\n",
        "    and Cloud models into a device-to-Cloud model cascade, and demonstrated 2\n",
        "    forms of routing to decide which model to use for a particular query.\n",
        "    Whereas the examples shown in these 2 tutorials used an unprotected Cloud\n",
        "    model and weren't focusing on privacy, here the hybrid approach can enable\n",
        "    us to combine the on-device and TEE-based genAI processing just mentioned\n",
        "    above to form a private device-to-TEE model cascade that's protected from\n",
        "    intrusion by untrusted external parties.\n",
        "\n",
        "If you haven't reviewed yet the tutorials mentioned above, we'd like to strongly\n",
        "encourage you to do so, as doing that will make the rest of this tutorial much\n",
        "easier to follow (we don't explain some of the foundational concepts and APIs in\n",
        "quite the same depth as we did in the tutorials above).\n",
        "\n",
        "## Overall architecture\n",
        "\n",
        "The overall architecture used in this tutorial will be a combination of what\n",
        "you've seen in [Tutorial 6](tutorial_6_confidential_computing.ipynb) and\n",
        "[Tutorial 8](tutorial_8_android.ipynb), except with a larger model in the TEE,\n",
        "and with hybrid processing mixed in.\n",
        "\n",
        "See the following diagram:\n",
        "\n",
        "![Private GenAI](private_genai.png)\n",
        "\n",
        "Let's go through this diagram step-by-step:\n",
        "\n",
        "*   We will be using two LLMs, both of them instances of\n",
        "    [Gemma](https://ai.google.dev/gemma). One will be a small quantized 2B that\n",
        "    comes in at approximately 1.5GB package and fits on a modern Android\n",
        "    device (e.g., on a Pixel 7). Another will be a much larger, unquantized 7B\n",
        "    version that comes in slightly over 34GB, and doesn't fit on a mobile\n",
        "    device, but that can be hosted in a TEE (e.g., on any of the upper end of\n",
        "    the\n",
        "    [N2D standard](https://cloud.google.com/compute/docs/general-purpose-machines#n2d-standard)\n",
        "    machine family, one of which we're going to use here to power this tutorial.\n",
        "    The two of them together will be used to power the GenAI workloads defined\n",
        "    in this tutorial.\n",
        "\n",
        "*   Both instances of Gemma will run in protected environments. One directly\n",
        "    bundled with the mobile app, the other in a TEE, with encrypted memory and\n",
        "    other strong assurances offered by the trusted computing environment. The\n",
        "    device and the TEE are connected with an encrypted communication channel to\n",
        "    form a whole. You can think of the TEE as effectively a logical extension\n",
        "    of the mobile device. The device confirms the identify of the workload\n",
        "    running in the TEE by obtaining and subsequently verifying an\n",
        "    [attestation](https://cloud.google.com/confidential-computing/confidential-vm/docs/attestation) report that contains a `SHA256` digest of\n",
        "    the image. The developer who builds the image locally, and knows what\n",
        "    the `SHA256` image digest should be, embeds the digest directly in the IR,\n",
        "    as an integral part of the GenAI workload.\n",
        "\n",
        "*   Interaction with the two Gemma instances is mediated by two instances of\n",
        "    GenC runtime, one on-device, and one in the TEE that talk to one-another\n",
        "    and jointly execute the GenAI workload you define. The on-device runtime\n",
        "    delegates processing to the runtime in the TEE in a manner defined by the\n",
        "    developer (you), directly based on the intent declared in the code.\n",
        "\n",
        "*   The developer's intent and the logic to execute, as always, are defined in\n",
        "    the form of portable\n",
        "    [Intermediate Representation (IR)](https://github.com/google/genc/tree/master/genc/docs/ir.md). We'll author the IR in the colab, then\n",
        "    deploy and run it on-device. GenC runtime instances use chunks of the\n",
        "    same IR to exchange the GenAI logic to be delegated from device to cloud.\n",
        "\n",
        "Keep in mind that in this particular deployment scenario, the protections do\n",
        "inherently rely on all the code involved (GenC runtime, the model, etc.) being\n",
        "open-source. Use of closed-source or untrusted code in this type of environment\n",
        "can be possible, but it is more complex, and falls outside the scope of this\n",
        "introductory tutorial.\n",
        "\n",
        "## Initial setup\n",
        "\n",
        "As usual, we start by ensuring that you have the development environment that\n",
        "can support the remainder of this tutorial.\n",
        "\n",
        "The best way to go about this is to review the setup steps in\n",
        "[Tutorial 6](tutorial_6_confidential_computing.ipynb) and\n",
        "[Tutorial 8](tutorial_8_android.ipynb).\n",
        "\n",
        "Rather than repeating the content of those tutorials, we'll limit ourselves\n",
        "here to a quick checklist. Please consult the above for the detailed steps.\n",
        "\n",
        "*   Follow the steps in\n",
        "    [SETUP.md](https://github.com/google/genc/tree/master/SETUP.md)\n",
        "    to download GenC from GitHub, build it, and run the tests in a docker\n",
        "    container. Make sure you can launch the Jupyter container, and then\n",
        "    connect to it and reopen this notebook in Jupyter, so that you can execute\n",
        "    all the Python code below.\n",
        "\n",
        "*   Setup for Android development (Developer mode on your Android device, USB\n",
        "    debugging, the `adb` tool on your development workstation, etc.), then\n",
        "    confirm you can build and install the `GenC Demo App` on your device, as\n",
        "    described in [Tutorial 8](tutorial_8_android.ipynb). We will continue to\n",
        "    use the same demo app to execute the GenAI logic authored in this tutorial.\n",
        "\n",
        "*   Setup a GCP project in which we can host a Confidential VM, as described\n",
        "    in [Tutorial 6](tutorial_6_confidential_computing.ipynb), install the\n",
        "    [`gcloud`](https://cloud.google.com/sdk/docs/install) command-line tool,\n",
        "    and then, as in [Tutorial 6](tutorial_6_confidential_computing.ipynb),\n",
        "    enter the\n",
        "    [`confidential_computing`](https://github.com/google/genc/tree/master/genc/cc/examples/confidential_computing)\n",
        "    directory, edit the\n",
        "    [config_environment.sh](https://github.com/google/genc/tree/master/genc/cc/examples/confidential_computing/config_environment.sh)\n",
        "    script there to match your GCP project, account names, etc.\n",
        "\n",
        "*   Obtain an instance of quantized Gemma 2B weights that you will use as an\n",
        "    on-device LLM. This ia a file named `gemma-2b-it-q4_k_m.gguf`, 1495245728 bytes in size. Push this file to your mobile device using the `adb` tool,\n",
        "    as discussed in [Tutorial 8](tutorial_8_android.ipynb), and take note of\n",
        "    the location. Also, download a copy into the docker container in which you\n",
        "    are doing the development and where you will be running the Jupyter\n",
        "    notebook instance, so we can play with it during development.\n",
        "\n",
        "Assuming you have all the above in place, there are a couple additional steps\n",
        "needed to upgrade the service image that runs in the TEE to use the larger\n",
        "Gemma 7B model.\n",
        "\n",
        "First, obtain an instance of unquantized Gemma 7B weights, e.g.,\n",
        "[from HuggingFace](https://huggingface.co/google/gemma-7b). Obtaining the file\n",
        "will require filling a form online, thus we can't auto-download it for you.\n",
        "You want to have a file named `gemma-7b-it.gguf` that is 34158344288 bytes in\n",
        "size before continuing.\n",
        "\n",
        "Next, copy this file to the\n",
        "[`confidential_computing`](https://github.com/google/genc/tree/master/genc/cc/examples/confidential_computing)\n",
        "directory that's to build images in\n",
        "[Tutorial 6](tutorial_6_confidential_computing.ipynb), such that it sits\n",
        "there side by side with the\n",
        "[`Dockerfile`](https://github.com/google/genc/tree/master/genc/cc/examples/confidential_computing/Dockerfile).\n",
        "\n",
        "Then, find this section of the `Dockerfile`:\n",
        "\n",
        "```\n",
        "RUN wget --directory-prefix=/ https://huggingface.co/lmstudio-ai/gemma-2b-it-GGUF/resolve/main/gemma-2b-it-q4_k_m.gguf\n",
        "```\n",
        "\n",
        "Remove the above, and instead insert the following:\n",
        "\n",
        "```\n",
        "COPY gemma-7b-it.gguf /gemma-7b-it.gguf\n",
        "```\n",
        "\n",
        "Now, you can build and push the image of the runtime that runs in the TEE, as\n",
        "described in [Tutorial 6](tutorial_6_confidential_computing.ipynb). The above\n",
        "change you made ensures that it's the larger Gemma 7B that\n",
        "gets bundled with the runtime. Go ahead and run\n",
        "`bash ./build_image.sh` and `bash ./push_image.sh` as described in\n",
        "[Tutorial 6](tutorial_6_confidential_computing.ipynb).\n",
        "\n",
        "Before you create VM by calling the `bash ./create_debug_vm.sh` script, you\n",
        "will want to tweak the VM parameters to grant more resources to accommodate\n",
        "the larger model, by editing the content of that script and inserting these\n",
        "two lines in place of the existing `machine_type` setting:\n",
        "\n",
        "```\n",
        "  --machine-type=\"n2d-standard-96\" \\\n",
        "  --boot-disk-size=\"100GB\" \\\n",
        "```\n",
        "\n",
        "With that, you should be able to run the script to create a debug VM, and since\n",
        "we're in debug mode, you can confirm it's up as discussed in\n",
        "[Tutorial 6](tutorial_6_confidential_computing.ipynb) by verifying that the VM\n",
        "has printed the `workload task started` at the end of the log in the serial\n",
        "console, and that there's no error message indicating that the VM has aborted.\n",
        "Keep in mind that due to the large image and file sizes involved, and you might\n",
        "see messages about the `systemd-journald.service` crashing and restarting. Wait\n",
        "until it resolves. This could take an hour, but it should eventually converge,\n",
        "and you should see the TEE instance coming up and reporting readiness as noted\n",
        "by the message quoted above.\n",
        "\n",
        "As long as there are no firewall rules in place that would prevent you from\n",
        "connecting to the VM from your device, you should be good to go.\n",
        "\n",
        "To conclude the setup, take a note of the SHA256 digest printed after uploading\n",
        "the image to GCP, and the external IP address of your convidential VM worker,\n",
        "enter them below, and then execute the following block of code before moving on\n",
        "to the next section."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "qOc0rqF9Guvl"
      },
      "outputs": [],
      "source": [
        "image_digest = \"\" # copy here the `sha256:SOMETHING` printed by ./push_image.sh\n",
        "server_ip = \"\" # copy here the `EXTERNAL IP` printed by ./create_debug_vm.sh\n",
        "\n",
        "server_address =  server_ip + \":80\"\n",
        "\n",
        "import genc\n",
        "from genc.python import authoring\n",
        "from genc.python import examples\n",
        "from genc.python import interop\n",
        "from genc.python import runtime\n",
        "\n",
        "genc.python.examples.executor.set_default_executor()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BicNh2iGHC1u"
      },
      "source": [
        "## Defining, deploying, and running your GenAI logic\n",
        "\n",
        "Now that everything is setup, let's start defining the logic you'll use.\n",
        "\n",
        "First, let's define all the models. For the smaller Gemma 2B, we'll define two\n",
        "versions, one for deployment on-device, and the other to load into the Jupyter\n",
        "notebook process, so that we can test the entire cascading setup from the\n",
        "comfort of the Jupyter notebook before we proceed to deployment onto the mobile\n",
        "device (which, as you will see, isn't particularly hard, but it does require\n",
        "jumping outside of the interactive Jupyter experience, dealing with the `adb`\n",
        "tool, bash scripts, USB cables, etc.).\n",
        "\n",
        "First, Gemma 2B on-device:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Im6-ZOpZM4zs"
      },
      "outputs": [],
      "source": [
        "@genc.python.authoring.traced_computation\n",
        "def gemma_2b_on_device(x):\n",
        "  return genc.python.authoring.model_inference_with_config[{\n",
        "      \"model_uri\": \"/device/gemma\",\n",
        "      \"model_config\": {\n",
        "          \"model_path\": \"/data/local/tmp/gemma-2b-it-q4_k_m.gguf\",\n",
        "          \"num_threads\" : 4,\n",
        "          \"max_tokens\" : 64}}](x)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "am7iUa3PNStH"
      },
      "source": [
        "Now, Gemma 7B in a TEE:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "PSPm_x7jNWHU"
      },
      "outputs": [],
      "source": [
        "@genc.python.authoring.traced_computation\n",
        "def gemma_7b_in_a_tee(x):\n",
        "  model = genc.python.authoring.model_inference_with_config[{\n",
        "      \"model_uri\": \"/device/gemma\",\n",
        "      \"model_config\": {\n",
        "          \"model_path\": \"/gemma-7b-it.gguf\",\n",
        "          \"num_threads\" : 64,\n",
        "          \"max_tokens\" : 64}}]\n",
        "  backend = {\"server_address\": server_address, \"image_digest\": image_digest}\n",
        "  return genc.python.authoring.confidential_computation[model, backend](x)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "s5j0OzVtNe-Y"
      },
      "source": [
        "Note that whereas the on-device Gemma 2B call was just a direct model call,\n",
        "modeled by a single `model_inference` operation, the call to a Gemma 7B in a\n",
        "TEE is mediated by the `confidential_computation` primitive that relays the\n",
        "specified processing (in this case, just a single model call) to the specified\n",
        "backend after first verifying the identity of the backend.\n",
        "\n",
        "Finally, here's Gemma 2B that we'll use in the notebook process for local\n",
        "testing, defined similar to the one on-device, but with the parameters slightly\n",
        "tweaked to reflect the model location and the available resources:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5bWr8Lt1QCY4"
      },
      "outputs": [],
      "source": [
        "@genc.python.authoring.traced_computation\n",
        "def gemma_2b_in_notebook(x):\n",
        "   return genc.python.authoring.model_inference_with_config[{\n",
        "      \"model_uri\": \"/device/gemma\",\n",
        "      \"model_config\": {\n",
        "          \"model_path\": \"/tmp/gemma-2b-it-q4_k_m.gguf\",\n",
        "          \"num_threads\" : 16,\n",
        "          \"max_tokens\" : 64}}](x)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GMo29EgVQKiu"
      },
      "source": [
        "Make sure that the three paths above match the locations where you uploaded the\n",
        "Gemma weights (both on Android, in the TEE as specified in the Dockerfile, and\n",
        "in the docker container in which you're running Jupyter), so that the GenC\n",
        "runtime can find them when executing the IR you define below."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zJORhQV3QhHW"
      },
      "source": [
        "Now, following ideas from [Tutorial 2](tutorial_2_custom_routing.ipynb), let's\n",
        "design a simple Boolean condition to route between a local model and a model\n",
        "in a TEE, and then use it to decide which model to call. Similarly to what we\n",
        "have done in [Tutorial 2](tutorial_2_custom_routing.ipynb), we'll prompt the\n",
        "small model (Gemma 2B) to make the determination, and the parse out the Boolean\n",
        "result from its response. We're going to define the scorer chain here as a\n",
        "function of the model used, so that we can construct a version of it to use in\n",
        "the Jupyter notebook, and a then again one setup for deployment on-device.\n",
        "\n",
        "To keep things simple for the small model, we'll ask it if the sentence is\n",
        "about cucumbers, and use that as the routing condition, on the hypothesis that\n",
        "the small model is particularly well suite for handling cucumber-themed input."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "CJxL32JMWWmK"
      },
      "outputs": [],
      "source": [
        "def make_scorer_chain(model_to_use_for_scoring):\n",
        "\n",
        "  prompt = \"\"\"\n",
        "  Read this text carefully: \"{x}\".\n",
        "  Did the text talk about cucumbers?\n",
        "  Please answer using words \"YES\" or \"NO\".\n",
        "  \"\"\"\n",
        "\n",
        "  regexp = \"YES|Yes|yes\"\n",
        "\n",
        "  @genc.python.authoring.traced_computation\n",
        "  def scorer_chain(x):\n",
        "    return genc.python.authoring.regex_partial_match[regexp](\n",
        "        model_to_use_for_scoring(\n",
        "            genc.python.authoring.prompt_template[prompt](x)))\n",
        "\n",
        "  return scorer_chain"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5YEEX67KYYgs"
      },
      "source": [
        "Let's try to see if it works with the Gemma 2B loaded into the notebook:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "__ir4D1CYdQ8"
      },
      "outputs": [],
      "source": [
        "chain = make_scorer_chain(gemma_2b_in_notebook)\n",
        "chain(\"The cucumbers I planted last summer didn't make it through the winter.\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eIxEvcdFc-9-"
      },
      "source": [
        "You should see:\n",
        "\n",
        "```\n",
        "True\n",
        "```"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lNsZTPNcdNHj"
      },
      "source": [
        "Now let's try in a different input:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "BgmPL1X9dQv7"
      },
      "outputs": [],
      "source": [
        "chain(\"When you see a bear, do not run and try to appear larger.\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qYCs6WTAdUjq"
      },
      "source": [
        "You should see:\n",
        "\n",
        "```\n",
        "False\n",
        "```"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aWGamVtgdXKa"
      },
      "source": [
        "Ok, now that the chain is working, we can use it to define a model cascade,\n",
        "similarly to what we did in [Tutorial 2](tutorial_2_custom_routing.ipynb).\n",
        "Once again, we'll define it as a function of the models used, so that we\n",
        "can play with it in the notebook.\n",
        "\n",
        "To vary the output and make it easier to tell which model is handling the\n",
        "query, we'll also alter the behavior. If it's a statement about cucumbers that\n",
        "we believe the small model should handle, we'll ask it to write a poem.\n",
        "Otherwise, we'll ask the larger model to angrily contradict everything we just\n",
        "said."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "lQoEghEUduxm"
      },
      "outputs": [],
      "source": [
        "def make_model_cascade(smaller_model, larger_model):\n",
        "\n",
        "  scorer_chain = make_scorer_chain(smaller_model)\n",
        "\n",
        "  @genc.python.authoring.traced_computation\n",
        "  def if_chain(x):\n",
        "    return smaller_model(\n",
        "        genc.python.authoring.prompt_template[\n",
        "            \"Write me a poem that starts with the following text: {x}\"](x))\n",
        "\n",
        "  @genc.python.authoring.traced_computation\n",
        "  def else_chain(x):\n",
        "    return larger_model(\n",
        "        genc.python.authoring.prompt_template[\n",
        "            \"Angrily contradict everything in the following text: {x}\"](x))\n",
        "\n",
        "  @genc.python.authoring.traced_computation\n",
        "  def model_cascade(x):\n",
        "    score = scorer_chain(x)\n",
        "    return genc.python.authoring.conditional[if_chain(x), else_chain(x)](score)\n",
        "\n",
        "  return model_cascade"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kx4SxweD3i7r"
      },
      "source": [
        "Now let's try it out on a cascade formed by the Gemma 2B in the Jupyter\n",
        "notebook, and the Gemma 7B in the TEE."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-55WyqoM1dcI"
      },
      "outputs": [],
      "source": [
        "cascade = make_model_cascade(gemma_2b_in_notebook, gemma_7b_in_a_tee)\n",
        "cascade(\"The cucumbers I planted last summer didn't make it through the winter.\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "x_kLGmE43evO"
      },
      "source": [
        "As it runs, you should see the debug printouts from llamacpp showing the model\n",
        "at work. In this case, it will be invoked twice, since the sentence is about\n",
        "cucumbers. Here's what this might look like:\n",
        "\n",
        "```\n",
        "I0000 00:00:1717128998.680136   32925 llamacpp.cc:135] Initial Prompt:\n",
        "  Read this text carefully: \"The cucumbers I planted last summer\n",
        "  didn't make it through the winter.\".\n",
        "  Did the text talk about cucumbers?\n",
        "  Please answer using words \"YES\" or \"NO\".\n",
        "  \n",
        "I0000 00:00:1717129020.039219   32925 llamacpp.cc:237]\n",
        "  **YES** the text talked about cucumbers.\n",
        "I0000 00:00:1717129020.039253   32925 llamacpp.cc:238]\n",
        "\n",
        "Decoded 11 tokens in 1.251856591s, speed: 8.78695 t/s\n",
        "I0000 00:00:1717129020.039742   33112 llamacpp.cc:135] Initial Prompt:\n",
        "Write me a poem that starts with the following text: The cucumbers\n",
        "I planted last summer didn't make it through the winter.\n",
        "I0000 00:00:1717129035.448336   33112 llamacpp.cc:237]\n",
        "\n",
        "**Cucumber Blues**\n",
        "\n",
        "The cucumbers I planted last summer didn't make it through the winter.\n",
        "Though the sun shone brightly,\n",
        "And the days were long and warm,\n",
        "I0000 00:00:1717129035.448377   33112 llamacpp.cc:238]\n",
        "\n",
        "Decoded 38 tokens in 4.300059072s, speed: 8.83709 t/s\n",
        "```"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "p5pWOkmT4G6x"
      },
      "source": [
        "So far so good, and let's try something that will trigger routing of the query\n",
        "to Gemma 7B running in a TEE:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "PCFPXqy11gTl"
      },
      "outputs": [],
      "source": [
        "cascade(\"When you see a bear, do not run and try to appear larger.\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CsYA1e8s4oef"
      },
      "source": [
        "Once again, you will see llamacpp debug output, but only one, since after the\n",
        "local scorer deetrmines the sentence is not about cucumbers, the remainder is\n",
        "routed to the Gemma 7B in a TEE. You can, however, switch to the log console\n",
        "in the TEE (since we're running a debug VM), to see those llamacpp logs as well.\n",
        "\n",
        "Here's what the local run might look like:\n",
        "\n",
        "```\n",
        "I0000 00:00:1717129251.777714   33844 llamacpp.cc:135] Initial Prompt:\n",
        "  Read this text carefully: \"When you see a bear, do not run and\n",
        "  try to appear larger.\".\n",
        "  Did the text talk about cucumbers?\n",
        "  Please answer using words \"YES\" or \"NO\".\n",
        "  \n",
        "I0000 00:00:1717129273.435469   33844 llamacpp.cc:237] The context\n",
        "does not mention cucumbers, so the answer is NO.\n",
        "I0000 00:00:1717129273.435500   33844 llamacpp.cc:238]\n",
        "\n",
        "Decoded 13 tokens in 1.480381989s, speed: 8.78152 t/s\n",
        "```\n",
        "\n",
        "And here's what you might see in the logs from llamacpp in the TEE:\n",
        "\n",
        "```\n",
        "I0000 00:00:1717129274.006131    3505 llamacpp.cc:135] Initial Prompt:\n",
        "Angrily contradict everything in the following text: When you see a bear,\n",
        "do not run and try to appear larger.\n",
        "I0000 00:00:1717129337.602353    3505 llamacpp.cc:237]  Instead, make\n",
        "yourself small and quiet, and back away slowly. If the bear sees you and\n",
        "feels threatened, it may charge at you. If you are caught in a bear's\n",
        "embrace, play\n",
        "I0000 00:00:1717129337.602385    3505 llamacpp.cc:238]\n",
        "\n",
        "Decoded 40 tokens in 13.409074718s, speed: 2.98305 t/s\n",
        "```"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9oNHcwCA55GZ"
      },
      "source": [
        "Now that we have verified that our cascade correctly decides\n",
        "which model to use and prompts the models as desired, we can\n",
        "move on to the on-device deployment.\n",
        "\n",
        "First, create a version of the cascade with the correctly configured Gemma 2B\n",
        "instance on-device, and save the IR to a local file, as follows."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "tKYsi9OD6Ojh"
      },
      "outputs": [],
      "source": [
        "cascade = make_model_cascade(gemma_2b_on_device, gemma_7b_in_a_tee)\n",
        "\n",
        "portable_ir = cascade.portable_ir\n",
        "\n",
        "with open(\"/tmp/genc_demo.pb\", \"wb\") as f:\n",
        "  f.write(portable_ir.SerializeToString())"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DfAtQgrq6_LO"
      },
      "source": [
        "Once that's done, use the `adb` tool to push the IR to the device, as you did\n",
        "in [Tutorial 8](tutorial_8_android.ipynb). Do not forget to install the\n",
        "`GenC Demo App` to run it in case you haven't done it during the initial setup.\n",
        "\n",
        "As a reminder, here are the highlights of what you may need to run inside the\n",
        "docker container:\n",
        "\n",
        "```\n",
        "bash ./setup_android_build_env.sh\n",
        "\n",
        "bazel build \\\n",
        "  --config=android_arm64 \\\n",
        "  genc/java/src/java/org/genc/examples/apps/gencdemo:app\n",
        "\n",
        "cp bazel-bin/genc/java/src/java/org/genc/examples/apps/gencdemo/app.apk .\n",
        "```\n",
        "\n",
        "And here what you may need to run outside of it, to setup your mobile phone:\n",
        "\n",
        "```\n",
        "adb install app.apk\n",
        "adb push genc_demo.pb /data/local/tmp/genc_demo.pb\n",
        "```\n",
        "\n",
        "And just as a sanity check:\n",
        "\n",
        "```\n",
        "adb ls /data/local/tmp\n",
        "\n",
        "000041f9 00000d7c 66595547 .\n",
        "000041e9 00000d7c 663eb4db ..\n",
        "000081b6 591fa3a0 65d68e51 gemma-2b-it-q4_k_m.gguf\n",
        "000081b6 00000643 665952d7 genc_demo.pb\n",
        "```\n",
        "\n",
        "With this, you can run the app and test some queries, first one that's related\n",
        "to cucumbers that triggers the routing decision to use only the small on-device\n",
        "Gemma 2B model to handle all processing:\n",
        "\n",
        "![Screenshot 1](tutorial_9_screenshot_1.png)\n",
        "\n",
        "And then, one that the small on-device Gemma 2B can recognize as not being\n",
        "about cucumbers, and that will cause our cascade to route the query to the larger Gemma 7B running in the TEE:\n",
        "\n",
        "![Screenshot 2](tutorial_9_screenshot_2.png)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "S7v0PkvvEZGW"
      },
      "source": [
        "To recap, you've seen how a smaller, faster, and cheaper Gemma 2B on-device can\n",
        "be combined with a larger and more capable, but slower and more expensive Gemma\n",
        "7B in a TEE to form a cascade that safely handles your sensitive queries, and\n",
        "that is able to take advantage of the unique strenghts of both depending on the\n",
        "context. Obviously, this was a silly example - we'll leave it as an exercise to\n",
        "the reader to come up with a more elaborate setup that makes sense in their\n",
        "specific problrm domain.\n",
        "\n",
        "Also, keep in mind that to support a real production deployment, you'd need to\n",
        "handle a range of aspects, from access control and authentication, through\n",
        "scaling, to use of more performant solutions than `llamacpp`, and potentially\n",
        "a judicious use of acceleration. The discussion of these topics is outside the\n",
        "scope of this introductory tutorial."
      ]
    }
  ],
  "metadata": {
    "colab": {
      "private_outputs": true,
      "provenance": []
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
