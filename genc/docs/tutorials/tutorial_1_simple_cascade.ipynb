{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Goy6i-1KOaAY"
      },
      "source": [
        "# Tutorial 1. Simple chain in LangChain powered by a device-to-Cloud model cascade\n",
        "\n",
        "This tutorial shows how to define simple application logic in LangChain in\n",
        "Python, use our interop APIs to configure it to be powered by a cascade of\n",
        "models that spans across the Gemini Pro model in Cloud and the Gemma model\n",
        "on-device, and migrate from running code in this Python notebook at a\n",
        "prototyping stage to deployment in the target environment - here, we'll use\n",
        "a generic Java client for simplicity's sake (but the same steps will apply\n",
        "for deployment on mobile platforms of your choice). This illustrates many\n",
        "of the key interoperability and portability benefits of GenC in one concise\n",
        "package. See the follow-up tutorials listed in the parent directory for how\n",
        "you can further extend and customize such logic to power more complex use\n",
        "cases."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Cr8gU1bxnMjA"
      },
      "source": [
        "## Initial setup\n",
        "\n",
        "Before we begin, we need to setup your environment, such that you can continue\n",
        "with the rest of this tutorial undisrupted.\n",
        "\n",
        "*   First, you need to start a Jupyter notebook with the GenC dependency\n",
        "    wired-in, and connect to that notebook - see\n",
        "    [SETUP.md](https://github.com/google/genc/tree/master/SETUP.md)\n",
        "    at the root of the repo, and the supporting files in the\n",
        "    [Jupyter setup directory](https://github.com/google/genc/tree/master/genc/docs/tutorials/jupyter_setup/)\n",
        "    for instructions how to setup the build and run environment and get Jupyter\n",
        "    up and running. To keep the setup simple and avoid dependency issues, we\n",
        "    will be running your Jupyter notebook in a docker instance.\n",
        "\n",
        "*   Next, you need to setup access to the Gemini Pro model that will be used in\n",
        "    this and follow-up tutorials as the Cloud model. In order to use this model,\n",
        "    you will need to obtain an API key and enter it in this notebook as a part\n",
        "    of the configuration. Please see the\n",
        "    [instructions](https://ai.google.dev/tutorials/rest_quickstart)\n",
        "    on how to get an API key to access this model through Google AI Studio.\n",
        "\n",
        "*   Finally, for the part of this tutorial that uses an on-device model, you\n",
        "    will need to obtain the\n",
        "    [e.g. Gemma 2B Quantized model](https://huggingface.co/lmstudio-ai/gemma-2b-it-GGUF/tree/main) file. For this tutorial, you'll want to fetch\n",
        "    `gemma-2b-it-q4_k_m.gguf`, and place it somewhere on your local filesystem\n",
        "    within the docker container, where you'll be running all the examples.\n",
        "    Keep note of the path where you download the model, since you'll need to\n",
        "    make sure later to supply the path to the model as a part of the tutorial to enable GenC to find and load it at runtime."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "A67WrPezqN2z"
      },
      "source": [
        "Now, to verify that GenC dependencies are loaded correctly, let's run a bunch\n",
        "of imports we're going to use later."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "tpMQqnD3qKIz"
      },
      "outputs": [],
      "source": [
        "import genc\n",
        "from genc.python import authoring\n",
        "from genc.python import interop\n",
        "from genc.python import runtime\n",
        "from genc.python import examples"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "z6Q-Cp9gorfw"
      },
      "source": [
        "## Defining application logic in LangChain\n",
        "\n",
        "We're going to create here an example application logic using LangChain APIs.\n",
        "For the sake of simplicy, let's go with a simple chain that consists of a prompt\n",
        "template feeding into an LLM call. Let's define it as a function, so that we can\n",
        "later play with different models. We're defining it as a Python function that's\n",
        "parameterized by the model, because we'll want to test different versions of it\n",
        "as we swap different models later on."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "MOihL6fbpTE8"
      },
      "outputs": [],
      "source": [
        "import langchain\n",
        "from langchain.prompts import PromptTemplate\n",
        "\n",
        "def create_my_chain(llm):\n",
        "  return langchain.chains.LLMChain(\n",
        "      llm=llm,\n",
        "      prompt=PromptTemplate(\n",
        "          input_variables=[\"topic\"],\n",
        "          template=\"Tell me about {topic}?\",\n",
        "      ),\n",
        "  )"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "e4Sr0EXyrEwf"
      },
      "source": [
        "## Declaring a Cloud model you will use to power the chain\n",
        "\n",
        "Now, let's define a model we can use. In GenC, we refer to models symbolically\n",
        "since the same model may be provisioned differently depending on where you run\n",
        "the code (recall that we want to demonstrate in this tutorial is running your\n",
        "application logic in this Jupyter notebook first, but then porting it to run\n",
        "elsewhere, possibly on a different platform, where the mechanism used to access\n",
        "your model may vary). To facilitate this, GenC provides interop APIs that\n",
        "enable you to declare the use of a model, e.g., as shown below. As noted\n",
        "earlier, for this tutorial, we're going to use the Gemini Pro model from Google\n",
        "Studio AI.\n",
        "\n",
        "NOTE: Please make sure you have an API_KEY to use Gemini Pro, as covered in the\n",
        "initial setup section above."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "pPTN8y8gHSvh"
      },
      "outputs": [],
      "source": [
        "API_KEY = \"\"  #@param\n",
        "\n",
        "my_cloud_model = genc.python.interop.langchain.CustomModel(\n",
        "    uri=\"/cloud/gemini\",\n",
        "    config=genc.python.interop.gemini.create_config(API_KEY))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "beCPCSZCR-JO"
      },
      "source": [
        "Now, you can construct the chain with it:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "IlPZJSaNSTtr"
      },
      "outputs": [],
      "source": [
        "my_chain = create_my_chain(my_cloud_model)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "W4M8y-PYTbzE"
      },
      "source": [
        "## Generating portable intermediate representation (IR)\n",
        "\n",
        "Now that you have the application logic (the chain you defined above), we need\n",
        "to translate it into what we call a *portable intermediate representation* (IR\n",
        "for short) that can be deployed in the target application environment. You do\n",
        "this by calling the converstion function provided by GenC, as follows:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "jfxHpZ6Qsqcg"
      },
      "outputs": [],
      "source": [
        "my_portable_ir = genc.python.interop.langchain.create_computation(my_chain)\n",
        "print(my_portable_ir)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RRszCjdZTwqT"
      },
      "source": [
        "At the time of this writing, this converter only supports a subset of LangChain\n",
        "functionality; we'll work to augment the coverage over time (and we welcome your\n",
        "help if there's a feature of LangChain you'd like to see covered and are willing\n",
        "to contribute it to the platform)."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_5sTduPZUHyt"
      },
      "source": [
        "## Testing the IR locally in the Jupyter notebook environment\n",
        "\n",
        "Before we move over to deployment on the client, let's first test that the IR is\n",
        "indeed working. While our goal is to run it on-device, we can just as well run\n",
        "it here, in this Jupyter notebook (remember, all the code is portable). To do\n",
        "this, we first need to construct a runtime instance:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "FgImBVLEUcGg"
      },
      "outputs": [],
      "source": [
        "my_runtime = genc.python.examples.executor.create_default_executor()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sD3KJhT_Udhc"
      },
      "source": [
        "Now, the constructor above is provided for convenience in running the examples\n",
        "and tutorials, and is configured with a number of runtime capabilities that we\n",
        "use in this context. Runtimes in GenC are fully modular and configurable, and\n",
        "in most advanced uses, you'll want to configure a runtime that suits the\n",
        "specific environment you want to run in, or your particular application (e.g.,\n",
        "with additional custom dependencies, or without certain dependencies you don't\n",
        "want in your environment). One of the tutorials later in the sequence explains\n",
        "how to do that. For now, the default example runtime will suffice."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4RUATRQOVJq5"
      },
      "source": [
        "Given the runtime and the portable IR we want to run, we can construct a\n",
        "*runner* object that will act like an ordinary Python function, and can\n",
        "be directly invoked, like this:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "KwFnhO-ns3wt"
      },
      "outputs": [],
      "source": [
        "my_runner = genc.python.runtime.Runner(my_portable_ir, my_runtime)\n",
        "\n",
        "print(my_runner(\"scuba diving\"))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bjhes-TSEHyW"
      },
      "source": [
        "## Adding an on-device model\n",
        "\n",
        "Now, recall earlier that we promised to form a cascade of two models, one of\n",
        "them being an on-device model. You can define an on-device model similarly to\n",
        "how you declared the cloud model, as shown below.\n",
        "\n",
        "NOTE: Make sure you have the model downloaded as per the setup instrucitons\n",
        "above, and update the PATH below to match."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4OblayZREpuF"
      },
      "outputs": [],
      "source": [
        "MODEL_PATH = \"/tmp/gemma-2b-it-q4_k_m.gguf\"\n",
        "\n",
        "my_on_device_model = genc.python.interop.langchain.CustomModel(\n",
        "      uri = \"/device/gemma\",\n",
        "      config = {\"model_path\" : MODEL_PATH,\n",
        "                \"num_threads\" : 4,\n",
        "                \"max_tokens\" : 64})"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4bR6Mp1jFHTB"
      },
      "source": [
        "Now, just like you did with the Cloud moel, you can construct the chain with\n",
        "your on-device model, generate the IR, and run it in your Colab notebook to\n",
        "test that it works:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "cO8O7aITFP2F"
      },
      "outputs": [],
      "source": [
        "my_chain = create_my_chain(my_on_device_model)\n",
        "my_portable_ir = genc.python.interop.langchain.create_computation(my_chain)\n",
        "my_runner = genc.python.runtime.Runner(my_portable_ir, my_runtime)\n",
        "print(my_runner(\"scuba diving\"))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "V0cGw1xSiRHw"
      },
      "source": [
        "## Forming a model cascade\n",
        "\n",
        "Now that you conformed each of your models is working, you can combine them\n",
        "into a model cascade, as shown below. For simplicity's sake, we'll start with\n",
        "a cascade that first tries to hit the cloud model, and falls back to the\n",
        "on-device model if the cloud model is unavailable. You can declare it as snown\n",
        "below."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3GbfpvMjiRHw"
      },
      "outputs": [],
      "source": [
        "my_model_cascade = genc.python.interop.langchain.ModelCascade(models=[\n",
        "    my_cloud_model, my_on_device_model])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WA8XKoV0GQd9"
      },
      "source": [
        "With that, you can use the model cascase as a parameter to your chain,\n",
        "reconstruct the IR with your logic powered by the cascade, and test that the\n",
        "setup still works, as follows:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "kq4B92gPGWbr"
      },
      "outputs": [],
      "source": [
        "my_chain = create_my_chain(my_model_cascade)\n",
        "my_portable_ir = genc.python.interop.langchain.create_computation(my_chain)\n",
        "my_runner = genc.python.runtime.Runner(my_portable_ir, my_runtime)\n",
        "print(my_runner(\"scuba diving\"))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xRpYfYPMiRHw"
      },
      "source": [
        "You could've chosen to order models in the cascade differently to achieve a\n",
        "different behavior. Everything is customizable! In the next tutorial in the\n",
        "sequence, we'll show you how you can construct an even more powerful routing\n",
        "mechanism, where routing is based on query sensitivity. For now, this simple\n",
        "cascade will suffice."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tYTmy5Y7Vdvz"
      },
      "source": [
        "## Saving the IR to a file for deployment\n",
        "\n",
        "Now that you have the full setup with a model cascade, and tested the IR\n",
        "locally, it's time to deploy it on the Java client as promised, and test it\n",
        "there. First, let's save the IR into a file on the local filesystem."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "njam4ZuNoiJ6"
      },
      "outputs": [],
      "source": [
        "with open(\"/tmp/genc_demo.pb\", \"wb\") as f:\n",
        "  f.write(my_portable_ir.SerializeToString())"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GhLV1BlXt0fK"
      },
      "source": [
        "## Deployment in a Java client in the target environment\n",
        "\n",
        "The portable IR you saved to a file above can be deployed on a variety\n",
        "of platforms, for for simplicity's sake, here's we'll just use a simple\n",
        "Java client process that you can run from the command-line, either on\n",
        "the same or a different machine. In the near future, we'll follow up\n",
        "with separate tutorials for loading the IR on specific mobile platforms\n",
        "to illustrate the portability benefits. Aside from the initial setup\n",
        "that may vary per-platform, the overall process looks roughly the same.\n",
        "\n",
        "If you look at the code in [GencDemo.java](https://github.com/google/genc/tree/master/genc/java/src/java/org/genc/examples/GencDemo.java), you\n",
        "will see a sequence similar to what you saw above in Python, but now\n",
        "expressed in Java APIs. The steps are always the same: first, you provision\n",
        "the portable IR (here loaded from a local file, the path to which is given\n",
        "as the first argument), next you construct an excutor that will run the IR\n",
        "(here using the default we supplied for use with the tutorials), next you\n",
        "create an call Java *runner* object that combines the two, and call it with\n",
        "the prompt (passed as the second command-line argument to the Java client).\n",
        "\n",
        "```\n",
        "Value ir = Constructor.readComputationFromFile(args[0]);\n",
        "DefaultExecutor executor = new DefaultExecutor();\n",
        "Runner runner = Runner.create(ir, executor.getExecutorHandle());\n",
        "String result = runner.call(args[1]);\n",
        "System.out.println(result);\n",
        "```\n",
        "\n",
        "If you're already running this notebook inside of a docker container and\n",
        "followed the steps in\n",
        "[SETUP.md](https://github.com/google/genc/tree/master/SETUP.md)\n",
        "then you have a build environment already setup. You can build and run the Java\n",
        "client as follows:\n",
        "\n",
        "```\n",
        "bazel run genc/java/src/java/org/genc/examples:genc_demo /tmp/genc_demo.pb scuba\n",
        "```"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "u1wvNNn3cp2E"
      },
      "source": [
        "This concludes the first tutorial."
      ]
    }
  ],
  "metadata": {
    "colab": {
      "last_runtime": {
        "build_target": "",
        "kind": "local"
      },
      "private_outputs": true,
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
