{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Goy6i-1KOaAY"
      },
      "source": [
        "# Tutorial 1. Simple chain in LangChain powered by a device-to-Cloud model cascade\n",
        "\n",
        "This tutorial shows how to define simple application logic in LangChain, use our\n",
        "interop APIs to configure it to be powered by a cascade of models that spans\n",
        "across a model in Cloud and an on-device model, and migrate from running code in\n",
        "this Python notebook to deployment in a Java client (e.g., a mobile app). This\n",
        "illustrates many of the key interoperability and portability benefits of GenC in\n",
        "one concise package. See the follow-up tutorials listed in\n",
        "the parent directory for how you can further extend and customize such logic to\n",
        "power more complex use cases."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Cr8gU1bxnMjA"
      },
      "source": [
        "## Initial setup\n",
        "\n",
        "Before we begin, we need to setup your environment, such that you can continue\n",
        "with the rest of this tutorial undisrupted.\n",
        "\n",
        "*   First, you need to start a Jupyter notebook with the GenC dependency\n",
        "    wired-in, and connect to that notebook - see\n",
        "    [SETUP.md](https://github.com/google/genc/tree/master/SETUP.md)\n",
        "    at the root of the repo, and the supporting files in the\n",
        "    [Jupyter setup directory](https://github.com/google/genc/tree/master/genc/docs/tutorials/jupyter_setup/)\n",
        "    for instructions how to setup the build and run environment and get Jupyter\n",
        "    up and running.\n",
        "\n",
        "*   Next, you need to setup access to the Gemini Pro model that will be used\n",
        "    in the tutorials. Please see the\n",
        "    [instructions](https://ai.google.dev/tutorials/rest_quickstart)\n",
        "    on how to get an API key to access this model through Google AI Studio.\n",
        "\n",
        "*   Finally, for the last portion of the tutorial that includes deployment on\n",
        "    a mobile client in Java, you will need to obtain a local model for your\n",
        "    device. Please see\n",
        "    [models.md](https://github.com/google/genc/tree/master/genc/docs/models.md)\n",
        "    for information on how to obtain models and\n",
        "    what backends to use. This tutorial supports running your model using\n",
        "    MediaPipe (optimized GPU performance, but a limited set of models) or\n",
        "    LlamaCpp (CPU-only, but many models supported). Once you have your model,\n",
        "    you'll need to push the model file to your device.\n",
        "\n",
        "    For example, for a MediaPipe Gemma model, you might need to fetch the\n",
        "    `gemma-2b-it-gpu-int4.bin` file, wherease for a LlamaCpp model\n",
        "    [e.g. Gemma 2B Quantized model](https://huggingface.co/lmstudio-ai/gemma-2b-it-GGUF/tree/main), you might need to fetch\n",
        "    `gemma-2b-it-q4_k_m.gguf`, and place either file in a location appropriate\n",
        "    for the client platform (e.g., in `/data/local/tmp/llm/` or similar).\n",
        "\n",
        "    Keep note of the path where you upload your model, since you'll need to\n",
        "    make sure to setup the Java client to load the model from that path later."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "A67WrPezqN2z"
      },
      "source": [
        "Now, to verify that GenC dependencies are loaded correctly, let's run a bunch\n",
        "of imports we're going to use later."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "tpMQqnD3qKIz"
      },
      "outputs": [],
      "source": [
        "import genc\n",
        "from genc import authoring\n",
        "from genc import interop\n",
        "from genc import runtime\n",
        "from genc import examples"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "z6Q-Cp9gorfw"
      },
      "source": [
        "## Defining application logic in LangChain\n",
        "\n",
        "We're going to create here an example application logic using LangChain APIs.\n",
        "For the sake of simplicy, let's go with a simple chain that consists of a prompt\n",
        "template feeding into an LLM call. Let's define it as a function, so that we can\n",
        "later play with different models."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "MOihL6fbpTE8"
      },
      "outputs": [],
      "source": [
        "import langchain\n",
        "from langchain.prompts import PromptTemplate\n",
        "\n",
        "def create_my_chain(llm):\n",
        "  return langchain.chains.LLMChain(\n",
        "      llm=llm,\n",
        "      prompt=PromptTemplate(\n",
        "          input_variables=[\"topic\"],\n",
        "          template=\"Tell me about {topic}?\",\n",
        "      ),\n",
        "  )"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "e4Sr0EXyrEwf"
      },
      "source": [
        "## Declaring a Cloud model you will use to power the chain\n",
        "\n",
        "Now, let's define a model we can use. In GenC, we refer to models symbolically\n",
        "since the same model may be provisioned differently depending on where you run\n",
        "the code (recall that we want to demonstrate in this tutorial is running your\n",
        "application logic in this Jupyter notebook first, but then porting it to run\n",
        "elsewhere, e.g., in this case on a Java client). To facilitate this, GenC\n",
        "provides interop APIs that enable you to declare the use of a model, e.g., as\n",
        "shown below. For this tutorial, we're going to use the Gemini Pro model from\n",
        "Google Studio AI.\n",
        "\n",
        "NOTE: Please make sure you have an API_KEY to use as covered in the \"Initial setup\" section (above)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "pPTN8y8gHSvh"
      },
      "outputs": [],
      "source": [
        "API_KEY = \"\"  #@param\n",
        "\n",
        "my_cloud_model = genc.interop.langchain.CustomModel(\n",
        "    uri=\"/cloud/gemini\",\n",
        "    config=genc.interop.gemini.create_config(API_KEY))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "beCPCSZCR-JO"
      },
      "source": [
        "Now, you can construct the chain with it:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "IlPZJSaNSTtr"
      },
      "outputs": [],
      "source": [
        "my_chain = create_my_chain(my_cloud_model)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "W4M8y-PYTbzE"
      },
      "source": [
        "## Generating portable intermediate representation (IR)\n",
        "\n",
        "Now that you have the application logic (the chain you defined above), we need\n",
        "to translate it into what we call a *portable intermediate representation* (IR\n",
        "for short) that can be deployed in the target application environment. You do\n",
        "this by calling the converstion function provided by GenC, as follows:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "jfxHpZ6Qsqcg"
      },
      "outputs": [],
      "source": [
        "my_portable_ir = genc.interop.langchain.create_computation(my_chain)\n",
        "print(my_portable_ir)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RRszCjdZTwqT"
      },
      "source": [
        "At the time of this writing, this converter only supports a subset of LangChain\n",
        "functionality; we'll work to augment the coverage over time (and we welcome your\n",
        "help if there's a feature of LangChain you'd like to see covered and are willing\n",
        "to contribute it to the platform)."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_5sTduPZUHyt"
      },
      "source": [
        "## Testing the IR locally in the Jupyter notebook environment\n",
        "\n",
        "Before we move over to deployment on the client, let's first test that the IR is\n",
        "indeed working. While our goal is to run it on-device, we can just as well run\n",
        "it here, in this Jupyter notebook (remember, all the code is portable). To do\n",
        "this, we first need to construct a runtime instance:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "FgImBVLEUcGg"
      },
      "outputs": [],
      "source": [
        "my_runtime = genc.examples.executor.create_default_executor()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sD3KJhT_Udhc"
      },
      "source": [
        "Now, the constructor above is provided for convenience in running the examples\n",
        "and tutorials, and is configured with a number of runtime capabilities that we\n",
        "use in this context. Runtimes in GenC are fully modular and configurable, and\n",
        "in most advanced uses, you'll want to configure a runtime that suits the\n",
        "specific environment you want to run in, or your particular application (e.g.,\n",
        "with additional custom dependencies, or without certain dependencies you don't\n",
        "want in your environment). One of the tutorials later in the sequence explains\n",
        "how to do that. For now, the default example runtime will suffice."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4RUATRQOVJq5"
      },
      "source": [
        "Given the runtime and the portable IR we want to run, we can construct a\n",
        "*runner* object that will act like an ordinary Python function, and can\n",
        "be directly invoked, like this:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "KwFnhO-ns3wt"
      },
      "outputs": [],
      "source": [
        "my_runner = genc.runtime.Runner(my_portable_ir, my_runtime)\n",
        "\n",
        "print(my_runner(\"scuba diving\"))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ArWgVKTn_hzC"
      },
      "source": [
        "Because of the portability of the IR, at this point you could deploy this IR\n",
        "as-is to your target platform, and test it there without the need for any\n",
        "changes, running the query with just the cloud model. One of GenC's goals is to\n",
        "enable easy experimentation and an iterative style of development, where you\n",
        "can check the results of your work at each step. To do this, jump to the\n",
        "section below entitled \"Saving the IR to a file for deployment\" for instructions\n",
        "on deployment.\n",
        "\n",
        "Otherwise, if you'd rather prefer to keep expanding on the existing logic, and\n",
        "deploy the final version of the IR just once at the end, continue on to the next\n",
        "section, where you will add a device model and run the complete model cascade."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "V0cGw1xSiRHw"
      },
      "source": [
        "## Adding an on-device model to form a model cascade\n",
        "\n",
        "Now, recall that what we promised to demonstrate in this tutorial running your\n",
        "application logic on a Java client (e.g., a mobile app), where it might be\n",
        "powered by an on-device LLM while the the device might be offline. To achieve\n",
        "this, we're going to need to modify the IR to include the on-device model.\n",
        "First, let's declare the use of an on-device model in LangChain, similarly to\n",
        "how we did above for the cloud model.\n",
        "\n",
        "NOTE: Make sure you have downloaded the on-device model as covered in the\n",
        "\"Initial setup\" section above. You'll need to provide the backend and\n",
        "model_path below."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "xay1QiN6iRHw"
      },
      "outputs": [],
      "source": [
        "from enum import Enum\n",
        "class LocalBackend(Enum):\n",
        "  MEDIAPIPE = 1\n",
        "  LLAMACPP = 2\n",
        "\n",
        "# Change these values based on your desired backend and model\n",
        "BACKEND = LocalBackend.MEDIAPIPE\n",
        "MODEL_PATH = \"/data/local/tmp/llm/gemma-2b-it-gpu-int4.bin\"\n",
        "\n",
        "# Create IR for on device model\n",
        "if (BACKEND == LocalBackend.MEDIAPIPE):\n",
        "  my_on_device_model = genc.interop.langchain.CustomModel(\n",
        "      uri = \"/device/llm_inference\",\n",
        "      config = {\"model_path\": MODEL_PATH,\n",
        "                \"max_tokens\": 64,\n",
        "                \"top_k\": 40,\n",
        "                \"temperature\": 0.8,\n",
        "                \"random_seed\": 100})\n",
        "elif (BACKEND == LocalBackend.LLAMACPP):\n",
        "    my_on_device_model = genc.interop.langchain.CustomModel(\n",
        "      uri = \"/device/llamacpp\",\n",
        "      config = {\"model_path\" : MODEL_PATH,\n",
        "                \"num_threads\" : 4,\n",
        "                \"max_tokens\" : 64})"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8jlOValUiRHw"
      },
      "source": [
        "Now, we're going to combine the cloud and on-device models into a simple type\n",
        "of model cascade that spans across cloud and on-device LLMs. For simplicity's\n",
        "sake, let's define a two-model cascade that first tries to hit a cloud backend\n",
        "in case we're online, and that defaults to the use of an on-device model when offline."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3GbfpvMjiRHw"
      },
      "outputs": [],
      "source": [
        "my_model_cascade = genc.interop.langchain.ModelCascade(models=[\n",
        "    my_cloud_model, my_on_device_model])\n",
        "\n",
        "my_chain = create_my_chain(my_model_cascade)\n",
        "my_portable_ir = genc.interop.langchain.create_computation(my_chain)\n",
        "print(my_portable_ir)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xRpYfYPMiRHw"
      },
      "source": [
        "You could've chosen to order models in the cascade differently to achieve a\n",
        "different behavior. Everything is customizable! In the next tutorial in the\n",
        "sequence, we'll show you how you can construct an even more powerful routing\n",
        "mechanism, where routing is based on query sensitivity. For now, this simple\n",
        "cascade will suffice."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tYTmy5Y7Vdvz"
      },
      "source": [
        "## Saving the IR to a file for deployment\n",
        "\n",
        "Now that you tested the IR locally, it's time to deploy it on the Java client\n",
        "as promised, and test it there. First, let's save the IR into a file on the\n",
        "local filesystem."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "njam4ZuNoiJ6"
      },
      "outputs": [],
      "source": [
        "with open(\"/tmp/genc_demo.pb\", \"wb\") as f:\n",
        "  f.write(my_portable_ir.SerializeToString())"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GhLV1BlXt0fK"
      },
      "source": [
        "## Deployment in a Java client in the target environment\n",
        "\n",
        "### Building/Deploying the Java client in Docker\n",
        "\n",
        "If you're already running this notebook inside of a docker container and\n",
        "followed the steps in\n",
        "[SETUP.md](https://github.com/google/genc/tree/master/SETUP.md)\n",
        "then you have a build environment already setup.\n",
        "\n",
        "You can build the client as follows:\n",
        "\n",
        "```\n",
        "bazel build genc/java/src/java/org/genc/examples/gencdemo\n",
        "```\n",
        "\n",
        "Once built..."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DtmVl-WHtF_T"
      },
      "source": [
        "## Run the demo\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7iLVUCbRaLMD"
      },
      "source": [
        "TODO"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "last_runtime": {
        "build_target": "",
        "kind": "local"
      },
      "private_outputs": true,
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
