{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8rBGCGFlXf_I"
      },
      "source": [
        "# Tutorial 3: Customizing the Runtime\n",
        "\n",
        "In addition to a modular authoring surface, the runtime is also modular and can\n",
        "be customized for your own platform, deployment environment, or product domain.\n",
        "In this tutorial, we explore the following topics:\n",
        "\n",
        "*   **Custom functions**. You can define an arbirary function, and make it\n",
        "    available as a building block in GenC that we can reference from the IR,\n",
        "    without the need for plumbing required to make it a full-blown reusable\n",
        "    component included with the framework. This is often the way to go if\n",
        "    you just need a one-off for a rarely used feature. For this example, we\n",
        "    will define a custom formatter and parser for calling a JSON backend.\n",
        "\n",
        "*   **Custom operators**. If the feature you want to add is broadly applicable\n",
        "    to many scenarios, it's worth adding it to GenC as a resuable building block\n",
        "    for ease of use by others. We will walk you through the process of defining\n",
        "    a new custom operator that can be added to GenC's operator library. For\n",
        "    this example, we will walk you through the embedding of Wolfram (an external\n",
        "    service) as a reusable GenC component.\n",
        "\n",
        "*   **Custom model backends**. Whereas GenC includes integration with a handful\n",
        "    of on-device and cloud LLMs , you can also define your own model\n",
        "    backends. We'll show you how to do this.\n",
        "\n",
        "*   **Custom runtime**. We'll illustrate how the various customizations we have\n",
        "    covered in this tutorial can all be combined together to define a customized\n",
        "    runtime that you can use instead of the example runtime we provided to power\n",
        "    your specialized use cases.\n",
        "\n",
        "To motivate this reading, in the next tutorial, we'll use all these building\n",
        "blocks to create LLM-powered agents.\n",
        "\n",
        "NOTE: In addition to this tutorial, you might want to review, at minimum, the\n",
        "documentation on the extensibility APIs in\n",
        "[api.md](https://github.com/google/generative_computing/tree/master/generative_computing/docs/api.md),\n",
        "and the overview of architecture in\n",
        "[architecture.md](https://github.com/google/generative_computing/tree/master/generative_computing/docs/architecture.md).\n",
        "(You might also find it useful to skim over an overview of the IR in\n",
        "[ir.md](https://github.com/google/generative_computing/tree/master/generative_computing/docs/ir.md),\n",
        "and internal runtime concepts in\n",
        "[runtime.md](https://github.com/google/generative_computing/tree/master/generative_computing/docs/runtime.md),\n",
        "albeit these shouldn't be essential.)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eDBkA9GAWuWR"
      },
      "source": [
        "## Initial Setup\n",
        "\n",
        "Before proceeding, please follow the instructions in\n",
        "[Tutorial 1](https://github.com/google/generative_computing/tree/master/generative_computing/docs/tutorials/tutorial_1_simple_cascade.ipynb)\n",
        "to set up your environment, connect Jupyter, and run the command below to run\n",
        "the GenC imports we're going to use."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "h9fbjuhoWxxj"
      },
      "outputs": [],
      "source": [
        "import generative_computing.python as genc\n",
        "from generative_computing.python import authoring\n",
        "from generative_computing.python import interop\n",
        "from generative_computing.python import runtime\n",
        "from generative_computing.python.examples import executor"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rBiAszEwZUur"
      },
      "source": [
        "## Custom functions\n",
        "\n",
        "By a *custom function* we refer to a piece of code that you can provision to\n",
        "become available and callable from within the IR (logic expressed in GenC) at\n",
        "runtime, without needing to incorporate it as a reusable building block within\n",
        "the ecosystem. This is the easiest, and the most lightweight way of extending\n",
        "the platform to integrate your new function in a way that you can flexibly mix\n",
        "and match with all other native building blocks provided by GenC.\n",
        "\n",
        "For example, many LLM backends take a structured (such as JSON) input and\n",
        "output. However, in a typical model cascade or a \"chain\", we want the call to\n",
        "the model to be text-in-text-out, with only the raw prompt going in, and only\n",
        "the generated text output going out, and without the associated model-specific\n",
        "JSON boilerplate, such that it can be used as a part of a larger structure that\n",
        "requires a backend-independent consistent constract. This means, there are two\n",
        "helper components that one may want to define to supplement your backend calls:\n",
        "\n",
        "*   **Format JSON input**. Given text, format it into a JSON request that is\n",
        "    well-formed for the specific model backend you want to integrate with.\n",
        "\n",
        "*   **Parse JSON output**. Given a well-formed JSON response from your custom\n",
        "    backend, parse it into a text, such that it can be plugged into a generic\n",
        "    backend-independent model cascade or a chain.\n",
        "\n",
        "Both of these can be expressed as simple stateless C++ functions, each of which\n",
        "can exist and run on its own. To make them usable by GenC, we can use the\n",
        "existing `CustomFunction` building block that's designed to support this very\n",
        "pattern. Let's take a look how this is done."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "f6kaczFq8ZN8"
      },
      "source": [
        "### Define the function interface\n",
        "\n",
        "First, let's write the two plain C++ functions to represent the JSON formatting\n",
        "and parsing. We won't worry about the GenC plumbing yet, but we'll write these\n",
        "in a manner that makes it easy to integrate them later. Specifically, we'll make\n",
        "two provisions:\n",
        "\n",
        "1. First, we'll use the protocol buffer structure that GenC uses at runtime to\n",
        "   represent both the argument and the result.\n",
        "   This is the `Value` message (defined in `computation.proto`) that can carry\n",
        "   either IR, or raw payloads (numbers, strings, tensors, multimodal data).\n",
        "   This will facilitate easier integration into GenC runtime as well as make\n",
        "   the function chainable to other operator regardless of your data type. The\n",
        "   signatures will look as follows:\n",
        "   \n",
        "   * `static absl::StatusOr\u003cv0::Value\u003e GetTopCandidateAsText(v0::Value input);`\n",
        "\n",
        "   * `static absl::StatusOr\u003cv0::Value\u003e WrapTextAsInputJson(v0::Value input);`\n",
        "\n",
        "2. Second, we'll collect the code that registers these functions with the GenC\n",
        "   runtime (under symbolic names that we can later use to reference from within the IR), and put them into a single method, such that it's easy to\n",
        "   call it later as we put all the pieces together at the end of this tutorial.\n",
        "   This is captured in a method named `SetCustomFunctions` with the signature\n",
        "   shown below. The code (discussed below) relies on concepts discussed in the\n",
        "   extensibility API documentation in\n",
        "   [api.md](https://github.com/google/generative_computing/tree/master/generative_computing/docs/api.md)\n",
        "   that you might wish to review before proceeding through the rest of this\n",
        "   tutorial.\n",
        "\n",
        "  `static absl::Status SetCustomFunctions(intrinsics::CustomFunction::FunctionMap\u0026 fn_map);`\n",
        "\n",
        "The declarations mentioned above can be found under\n",
        "[`gemini_parser.h`](https://github.com/google/generative_computing/tree/master/generative_computing/cc/modules/parsers/gemini_parser.h),\n",
        "and the associated implementation in the associated C++ file\n",
        "[`gemini_parser.cc`](https://github.com/google/generative_computing/tree/master/generative_computing/cc/modules/parsers/gemini_parser.cc)\n",
        "in the same directory.\n",
        "\n",
        "```c++\n",
        "// Parsers for Gemini model.\n",
        "class GeminiParser final {\n",
        " public:\n",
        "  ~GeminiParser() = default;\n",
        "\n",
        "  // Extract Top Candidate as Text.\n",
        "  static absl::StatusOr\u003cv0::Value\u003e GetTopCandidateAsText(v0::Value input);\n",
        "\n",
        "  // Wraps a text as Gemini request JSON.\n",
        "  static absl::StatusOr\u003cv0::Value\u003e WrapTextAsInputJson(v0::Value input);\n",
        "\n",
        "  // Make Parser functions visible to the runtime.\n",
        "  static absl::Status SetCustomFunctions(\n",
        "      intrinsics::CustomFunction::FunctionMap\u0026 fn_map);\n",
        "\n",
        "  // Not copyable or movable.\n",
        "  GeminiParser(const GeminiParser\u0026) = delete;\n",
        "  GeminiParser\u0026 operator=(const GeminiParser\u0026) = delete;\n",
        "\n",
        " private:\n",
        "  // Do not hold states in this class.\n",
        "  GeminiParser() = default;\n",
        "};\n",
        "```"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EQnD3UCd-sIT"
      },
      "source": [
        "### Define the functions\n",
        "\n",
        "Here are the bodies of the two custom converter functions mentioned above. As\n",
        "you can see, there's nothing particularly GenC-specific here other than the use\n",
        "of the protocol buffer message (`v0::Value` below) that GenC uses to represent\n",
        "the IR as well as values in its runtime. As noted above, you can find the code under\n",
        "[`gemini_parser.cc`](https://github.com/google/generative_computing/tree/master/generative_computing/cc/modules/parsers/gemini_parser.cc).\n",
        "\n",
        "``` c++\n",
        "absl::StatusOr\u003cv0::Value\u003e GeminiParser::GetTopCandidateAsText(v0::Value input) {\n",
        "  auto parsed_json = nlohmann::json::parse(input.str(), /*cb=*/nullptr,\n",
        "                                           /*allow_exceptions=*/false);\n",
        "  if (parsed_json.is_discarded()) {\n",
        "    return absl::InternalError(absl::Substitute(\n",
        "        \"Failed parsing json output from Gemini: $0\", input.DebugString()));\n",
        "  }\n",
        "\n",
        "  std::string extract_first_candidate_as_text =\n",
        "      \"{% if candidates %}{% for p in candidates.0.content.parts \"\n",
        "      \"%}{{p.text}}{% endfor %}{%   endif %}\";\n",
        "\n",
        "  inja_status_or::Environment env;\n",
        "\n",
        "  v0::Value result;\n",
        "  std::string result_str =\n",
        "      GENC_TRY(env.render(extract_first_candidate_as_text, parsed_json));\n",
        "  result.set_str(result_str);\n",
        "  return result;\n",
        "}\n",
        "\n",
        "absl::StatusOr\u003cv0::Value\u003e GeminiParser::WrapTextAsInputJson(v0::Value input) {\n",
        "  std::string json_request = absl::Substitute(\n",
        "      R\"pb(\n",
        "        {\n",
        "          \"contents\":\n",
        "          [ {\n",
        "            \"parts\":\n",
        "            [ { \"text\": \"$0\" }]\n",
        "          }]\n",
        "        }\n",
        "      )pb\",\n",
        "      input.str());\n",
        "  v0::Value result;\n",
        "  result.set_str(json_request);\n",
        "  return result;\n",
        "}\n",
        "```"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-qDqFddi_QEU"
      },
      "source": [
        "### Make your functions visible to the runtime\n",
        "\n",
        "Now, onto the registration (shown below, and also found in\n",
        "[`gemini_parser.cc`](https://github.com/google/generative_computing/tree/master/generative_computing/cc/modules/parsers/gemini_parser.cc)).\n",
        "\n",
        "The parameter `FunctionMap` is a data structure defined in\n",
        "[`custom_function.h`](https://github.com/google/generative_computing/tree/master/generative_computing/cc/intrinsics/custom_function.h) and provided by GenC at runtime\n",
        "construction time, where the keys are symbolic names of the custom functions\n",
        "that you will reference in the IR, and values are C++ lambdas with their bodies.\n",
        "The symbolic names (e.g., `/gemini_parser/get_top_candidate_as_text` below)\n",
        "are totally up to you to define - you just have to make sure that the names\n",
        "registered with the runtime match those that you will use later when authoring\n",
        "your IR.\n",
        "\n",
        "```c++\n",
        "absl::Status GeminiParser::SetCustomFunctions(\n",
        "    intrinsics::CustomFunction::FunctionMap\u0026 fn_map) {\n",
        "  fn_map[\"/gemini_parser/get_top_candidate_as_text\"] =\n",
        "      [](const v0::Value\u0026 arg) {\n",
        "        return GeminiParser::GetTopCandidateAsText(arg);\n",
        "      };\n",
        "\n",
        "  fn_map[\"/gemini_parser/wrap_text_as_input_json\"] = [](const v0::Value\u0026 arg) {\n",
        "    return GeminiParser::WrapTextAsInputJson(arg);\n",
        "  };\n",
        "\n",
        "  return absl::OkStatus();\n",
        "}\n",
        "```"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nVHtUBklAdeZ"
      },
      "source": [
        "Towards the end of this tutorial, we'll show you how to setup a custom runtime\n",
        "where everything we define here comes together (you can find all that code in\n",
        "[executor_stacks.cc](https://github.com/google/generative_computing/tree/master/generative_computing/cc/examples/executors/executor_stacks.cc)).\n",
        "\n",
        "During the runtime setup, we'll be setting up a `config` object that, among a\n",
        "handful of other things, contains a `custom_function_map`. This is the same\n",
        "map that's declared as a formal parameter to the registration function you've\n",
        "just defined. We will invoke it there to register custom functions as shown\n",
        "below.\n",
        "\n",
        "```c++\n",
        "GENC_TRY(GeminiParser::SetCustomFunctions(config.custom_function_map));\n",
        "```\n",
        "\n",
        "As noted earlier, if you'd like to understand the concepts behind runtime\n",
        "customization before proceeding, take a look at extensibility APIs in\n",
        "[api.md](https://github.com/google/generative_computing/tree/master/generative_computing/docs/api.md).\n",
        "Otherwise, continue on to see how the custom functions you've defined are used\n",
        "when authoring the application logic in GenC."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GEycBIiUAiNB"
      },
      "source": [
        "### Authoring the IR that calls your custom functions\n",
        "\n",
        "Assuming the custom runtime with your custom functions is setup (as noted,\n",
        "we'll show you the full example at the end), here's how you can use them, along\n",
        "with the REST calls to your custom backend and other building blocks, to\n",
        "construct a chain that calls your backend and satisfies a simple\n",
        "backend-independent text-in-text-out contract.\n",
        "\n",
        "Since we have used Python as the authoring surface in the earlier tutorials,\n",
        "we're going to stick to Python here as well, but keep in mind that you could do\n",
        "this also in C++ (see\n",
        "[api.md](https://github.com/google/generative_computing/tree/master/generative_computing/docs/api.md)\n",
        "for details).\n",
        "\n",
        "In order to make things easier for you, the example runtime we setup to support\n",
        "the tutorials already has the custom functions defined above wired in, so that\n",
        "you can test authoring with your functions right away, as below.\n",
        "\n",
        "NOTE: You will need to fill in a correct Gemini endpoint address with API key\n",
        "in the code below (see the instructions at the beginning of\n",
        "[Tutorial 1](https://github.com/google/generative_computing/tree/master/generative_computing/docs/tutorials/tutorial_1_simple_cascade.ipynb)\n",
        "for how to get those if you don't have them yet). We included an example of\n",
        "what this may look like for an example model, but keep in mind this may change.\n",
        "You may also want to look at\n",
        "[math_tool_agent.py](https://github.com/google/generative_computing/tree/master/generative_computing/python/examples/math_tool_agent.py)\n",
        "(Python) and\n",
        "[run_gemini_on_ai_studio.cc](https://github.com/google/generative_computing/tree/master/generative_computing/cc/examples/run_gemini_on_ai_studio.cc)\n",
        "(C++) for similar examples that involve the use of Germini model and REST calls."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "xQzDTG3FBaE1"
      },
      "outputs": [],
      "source": [
        "# An example endpoint may look like below, but please verify as this can change\n",
        "# https://generativelanguage.googleapis.com/v1beta/models/gemini-pro:generateContent?key=XYZ\"\n",
        "rest_call = genc.authoring.create_rest_call(\"\u003cend point with api key\u003e\")\n",
        "str_to_json_request = genc.authoring.create_custom_function(\n",
        "    \"/gemini_parser/wrap_text_as_input_json\"\n",
        ")\n",
        "extrat_top_candidate = genc.authoring.create_custom_function(\n",
        "    \"/gemini_parser/get_top_candidate_as_text\"\n",
        ")\n",
        "\n",
        "my_backend_chain = (\n",
        "    genc.interop.langchain.CustomChain()\n",
        "    | str_to_json_request\n",
        "    | rest_call\n",
        "    | extrat_top_candidate\n",
        ")\n",
        "\n",
        "portable_ir = genc.interop.langchain.create_computation(my_backend_chain)\n",
        "executor = genc.examples.executor.create_default_executor()\n",
        "runner = genc.runtime.Runner(portable_ir, executor)\n",
        "runner(\"Tell me a story\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "f7ft3_qUZCBI"
      },
      "source": [
        "## Custom operators\n",
        "\n",
        "As noted above, a custom operator is a great way to capture logic that's likely\n",
        "to be used more than once, and worth capturing as a reusable component that you\n",
        "can either contribute to GenC, or keep in your own repo (and use to setup your\n",
        "specialized runtimes).\n",
        "\n",
        "In an GenAI application, we often use tools - for instance, WolframAlpha. This could fit as `CustomFunction`, but if you're going to use it often, you may\n",
        "want to be able to write code like `genc.authoring.create_wolfram_alpha(appid)`\n",
        "to make it more readily available, and to make it look and feel just like any\n",
        "of the other operators included with GenC."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "i2UaoqSsFmbu"
      },
      "source": [
        "### Declare a handler\n",
        "\n",
        "First, we need to define a handler - code that implements the custom operator\n",
        "and plugs into the runtime. It's implemented in C++, and inherits from one of\n",
        "two interfaces defined in\n",
        "[intrinsic_handler.h](https://github.com/google/generative_computing/tree/master/generative_computing/cc/runtime/intrinsic_handler.h).\n",
        "In this case we're going to derive from the base class\n",
        "`InlineIntrinsicHandlerBase`\n",
        "since the operator we're writing is a simple in-and-out type of processing,\n",
        "not a new control flow abstraction (see the section on extensibility APis in\n",
        "[api.md](https://github.com/google/generative_computing/tree/master/generative_computing/docs/api.md)\n",
        "and runtime documentation in\n",
        "[runtime.md](https://github.com/google/generative_computing/tree/master/generative_computing/docs/runtime.md)\n",
        "for a discussion of the different types of operators).\n",
        "\n",
        "Here's what the declaration would look like (full code in\n",
        "[wolfram_alpha.h](https://github.com/google/generative_computing/tree/master/generative_computing/cc/modules/tools/wolfram_alpha.h)):\n",
        "\n",
        "```c++\n",
        "// Tools for calling WolframAlpha API.\n",
        "class WolframAlpha : public InlineIntrinsicHandlerBase {\n",
        " public:\n",
        "  WolframAlpha() : InlineIntrinsicHandlerBase(kWolframAlpha) {}\n",
        "  virtual ~WolframAlpha() {}\n",
        "\n",
        "  absl::Status CheckWellFormed(const v0::Intrinsic\u0026 intrinsic_pb) const final;\n",
        "\n",
        "  absl::Status ExecuteCall(const v0::Intrinsic\u0026 intrinsic_pb,\n",
        "                           const v0::Value\u0026 arg, v0::Value* result) const final;\n",
        "};\n",
        "```"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ELGmi5uDJuTO"
      },
      "source": [
        "The method `ExecuteCall` is the one that defines the processing logic."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Wphj7xNLF6js"
      },
      "source": [
        "### Implement the handler logic\n",
        "\n",
        "Firts, here's an example implementation you might write in C++ without GenC.\n",
        "\n",
        "```c++\n",
        "// Callback fn to write the response.\n",
        "size_t WriteCallback(void* contents, size_t size, size_t nmemb,\n",
        "                     std::string* output) {\n",
        "  size_t totalSize = size * nmemb;\n",
        "  output-\u003eappend(static_cast\u003cchar*\u003e(contents), totalSize);\n",
        "  return totalSize;\n",
        "}\n",
        "\n",
        "// Calls Wolfram Alpha API to get a string response.\n",
        "absl::StatusOr\u003cstd::string\u003e CallShortAnswersAPI(const std::string\u0026 app_id,\n",
        "                                                const std::string\u0026 query) {\n",
        "  CURL* curl;\n",
        "  CURLcode curl_code;\n",
        "  std::string readBuffer;\n",
        "\n",
        "  curl = curl_easy_init();\n",
        "  if (curl == nullptr) return absl::InternalError(\"Unable to init CURL\");\n",
        "\n",
        "  char* escaped_query = curl_easy_escape(curl, query.c_str(), 0);\n",
        "  std::string url =\n",
        "      \"http://api.wolframalpha.com/v1/result?i=\" + std::string(escaped_query) +\n",
        "      \"\u0026appid=\" + app_id + \"\u0026output=json\u0026format=plaintext\";\n",
        "\n",
        "  curl_easy_setopt(curl, CURLOPT_URL, url.c_str());\n",
        "  curl_easy_setopt(curl, CURLOPT_WRITEFUNCTION, WriteCallback);\n",
        "  curl_easy_setopt(curl, CURLOPT_WRITEDATA, \u0026readBuffer);\n",
        "\n",
        "  curl_code = curl_easy_perform(curl);\n",
        "\n",
        "  // Error out if call fails\n",
        "  if (curl_code != CURLE_OK) {\n",
        "    return absl::InternalError(curl_easy_strerror(curl_code));\n",
        "  }\n",
        "  curl_easy_cleanup(curl);\n",
        "  return readBuffer;\n",
        "}\n",
        "```"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "55cTyISGGfvw"
      },
      "source": [
        "Now, here's how you can plug this implementation into the handler class you've\n",
        "defined earlier (full code in\n",
        "[wolfram_alpha.cc](https://github.com/google/generative_computing/tree/master/generative_computing/cc/modules/tools/wolfram_alpha.cc)).\n",
        "\n",
        "```c++\n",
        "absl::Status WolframAlpha::CheckWellFormed(\n",
        "    const v0::Intrinsic\u0026 intrinsic_pb) const {\n",
        "  if (!intrinsic_pb.static_parameter().has_str()) {\n",
        "    return absl::InvalidArgumentError(\"Expect template as appid, got none.\");\n",
        "  }\n",
        "  return absl::OkStatus();\n",
        "}\n",
        "\n",
        "absl::Status WolframAlpha::ExecuteCall(const v0::Intrinsic\u0026 intrinsic_pb,\n",
        "                                       const v0::Value\u0026 arg,\n",
        "                                       v0::Value* result) const {\n",
        "  const std::string\u0026 appid = intrinsic_pb.static_parameter().str();\n",
        "  std::string result_str = GENC_TRY(CallShortAnswersAPI(appid, arg.str()));\n",
        "  result-\u003eset_str(result_str);\n",
        "  return absl::OkStatus();\n",
        "}\n",
        "```"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QgE7JwcuLFik"
      },
      "source": [
        "### Make it known to the runtime\n",
        "\n",
        "Now, we have a C++ handler for the new operator, but the runtime doesn't know\n",
        "about it. Simiarly to custom functions earlier, we need to include in during\n",
        "the runtime construction process, and here again, we can do that by adding to\n",
        "the runtime `config` object, this time to the `custom_intrinsics_list`. You\n",
        "can do that as shown below (and as mentioned earlier, you will see this in the\n",
        "full context at the end of this tutorial).\n",
        "\n",
        "```c++\n",
        "config.custom_intrinsics_list.push_back(new intrinsics::WolframAlpha());\n",
        "```"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "W0m8uPkIHy79"
      },
      "source": [
        "### Write a constructor for authoring\n",
        "\n",
        "Now that we have a handler, and we have it wired into the runtime, we still\n",
        "need to create a corresponding piece of the authoring surface to make it into\n",
        "a fully functional building block, symmetric with the existing ones. You can\n",
        "do that by defining a *constructor* function in C++ (and subsequently lifting\n",
        "it into Python via `pybind11`). Constructors construct a piece of IR that\n",
        "represents your new operator by setting the `uri` field in the `Intrinsic`\n",
        "message, and populating any additional static parameters that may go in the IR\n",
        "along with your operator in the `static_parameter` field in the proto, as shown\n",
        "in the example code below (see also\n",
        "[ir.md](https://github.com/google/generative_computing/tree/master/generative_computing/docs/ir.md)\n",
        "for a more complete explanation of how operators are represented in the IR).\n",
        "\n",
        "```c++\n",
        "absl::StatusOr\u003cv0::Value\u003e CreateWolframAlpha(absl::string_view appid) {\n",
        "  v0::Value wolfram_alpha_pb;\n",
        "  v0::Intrinsic* const intrinsic_pb = wolfram_alpha_pb.mutable_intrinsic();\n",
        "  intrinsic_pb-\u003eset_uri(\"wolfram_alpha\"));\n",
        "  intrinsic_pb-\u003emutable_static_parameter()-\u003eset_str(std::string(appid));\n",
        "  intrinsic_pb-\u003emutable_static_parameter()-\u003eset_label(\"appid\");\n",
        "  return wolfram_alpha_pb;\n",
        "}\n",
        "```"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4p0RE7M6kY2y"
      },
      "source": [
        "Finally, to lift this constructor for use in Python, you need to augment the\n",
        "appropriate section in `pybind11` in\n",
        "[constructor_bindings.cc](https://github.com/google/generative_computing/tree/master/generative_computing/cc/authoring/constructor_bindings.cc), as shown in the snippet of code below:\n",
        "\n",
        "```c++\n",
        "m.def(\"create_wolfram_alpha\", \u0026CreateWolframAlpha,\n",
        "      \"Returns an operator that makes calls to WolframAlpha ShortAnswer API\");\n",
        "```"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "O0irtVQMNRVH"
      },
      "source": [
        "If you want to integrate it more tightly with GenC and include it by default,\n",
        "you migth also want to register it in\n",
        "[intrinsic_uris.h](https://github.com/google/generative_computing/tree/master/generative_computing/cc/intrinsics/intrinsic_uris.h) and include it in\n",
        "[handler_sets.cc](https://github.com/google/generative_computing/tree/master/generative_computing/cc/intrinsics/handler_sets.cc), but that's not required (and you wouldn't want to do this for\n",
        "an operator that's more specific to your domain or environment)."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9h1FMfnbG3uI"
      },
      "source": [
        "### Try it\n",
        "\n",
        "Once defined as per above, your operator can be used like any other (note you\n",
        "will need to populate the Wolfram appid to make this work):"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "w_ZKXpRlHCA-"
      },
      "outputs": [],
      "source": [
        "portable_ir = authoring.create_wolfram_alpha(\"\u003cyour appid\u003e\")\n",
        "executor = genc.examples.executor.create_default_executor()\n",
        "runner = runtime.Runner(portable_ir, executor)\n",
        "print(runner(\"what is the result of 2^2-2-3+4*100\"))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-m6PBZLwsKxx"
      },
      "source": [
        "## Model inference\n",
        "\n",
        "GenC provides a few predefined ways to call popular model backends such as ChatGPT and Gemini to get you started. In many applications, you will want to\n",
        "include custom model backends for your own specialized use cases.\n",
        "\n",
        "Among the many model backends out there (Gemini, ChatGPT, self-hosted LLAMA,\n",
        "and so on), not only the ways of calling them, but also the formats of inputs\n",
        "and outputs may vary. For example, as noted earlier, many backends interact\n",
        "with you through custom JSON blobs that vary across models.\n",
        "\n",
        "In order to promote composability, we want to make these models usable through\n",
        "a standardized protocol, such that, e.g., they can appear in a model cascade,\n",
        "or as a part of chains. This section shows how to do that.\n",
        "\n",
        "Recall that at the begining of this tutorial, we introduced a model call chain\n",
        "with parsers and formatters, and a custom REST backend call. Now, let's\n",
        "consider how to package all this as a standardized reusable \"model inference\"\n",
        "abstraction, by integrating the model input formatter, model call, and a model\n",
        "output parser into one operator that accepts and returns just the pure generic\n",
        "payloads (prompt or response string or multimodal data)."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SZIjUcLGEsi-"
      },
      "source": [
        "As in the previous examples, we're going to capture all this in a single class,\n",
        "as shown below (and see\n",
        "[google_ai.h](https://github.com/google/generative_computing/tree/master/generative_computing/cc/interop/backends/google_ai.h)\n",
        "and\n",
        "[google_ai.cc](https://github.com/google/generative_computing/tree/master/generative_computing/cc/interop/backends/google_ai.cc)\n",
        "for the full example).\n",
        "\n",
        "```c++\n",
        "class GoogleAI final {\n",
        " public:\n",
        "  ~GoogleAI() = default;\n",
        "\n",
        "  // Not copyable or movable.\n",
        "  GoogleAI(const GoogleAI\u0026) = delete;\n",
        "  GoogleAI\u0026 operator=(const GoogleAI\u0026) = delete;\n",
        "\n",
        "  // Sets the inference map to process model calls.\n",
        "  static absl::Status SetInferenceMap(\n",
        "      intrinsics::ModelInferenceWithConfig::InferenceMap\u0026 inference_map);\n",
        "\n",
        " private:\n",
        "  // Do not hold states in this class.\n",
        "  GoogleAI() = default;\n",
        "};\n",
        "}  // namespace generative_computing\n",
        "```\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "z479HNo0FVfm"
      },
      "source": [
        "The way this works is going to look, in many ways, somewhat similar to how we\n",
        "implemented custom functions and operators earlier in this tutorial:\n",
        "\n",
        "*   When constructing the runtime, we'll be setting up a `config` object in\n",
        "    C++ that, in this case, will have a field named\n",
        "    `model_inference_with_config_map` of interest to us. This is a map from\n",
        "    model names to C++ lambdas that call the corresponding custom backends.\n",
        "    This registration logic is captured the `SetInferenceMap` method we've\n",
        "    just declared above.\n",
        "\n",
        "*   The C++ lambda will accept and return `v0::Value` messages, as well as take\n",
        "    a message with the `static_parameter` that may contain additional config\n",
        "    embedded in the IR along with the reference to your model.\n",
        "\n",
        "See below a possible C++ implementation of the above.\n",
        "\n",
        "The details of the implementation aren't terribly important for this tutorial,\n",
        "but note how we use CURL (the model is sitting behind a REST endpoint), that's\n",
        "something you wight want to use as well.\n",
        "\n",
        "Note also in this case, it's the Gemini model we used in earlier tutorials,\n",
        "hence the use of the familar `/cloud/gemini` key in the inference map.\n",
        "\n",
        "```c++\n",
        "absl::Status GoogleAI::SetInferenceMap(\n",
        "    intrinsics::ModelInferenceWithConfig::InferenceMap\u0026 inference_map) {\n",
        "  inference_map[\"/cloud/gemini\"] =\n",
        "      [](v0::Intrinsic intrinsic, v0::Value arg) -\u003e absl::StatusOr\u003cv0::Value\u003e {\n",
        "    // Construct input JSON\n",
        "    std::string input_json = absl::Substitute(\n",
        "        R\"pb(\n",
        "          {\n",
        "            \"contents\":\n",
        "            [ {\n",
        "              \"parts\":\n",
        "              [ { \"text\": \"$0\" }]\n",
        "            }]\n",
        "          }\n",
        "        )pb\",\n",
        "        arg.str());\n",
        "\n",
        "    // Construct the REST calls parameters from config     \n",
        "    const v0::Value\u0026 config = intrinsic.static_parameter().struct_().element(1);\n",
        "    const std::string\u0026 endpoint =\n",
        "        config.struct_().element(0).str() + config.struct_().element(1).str();\n",
        "    std::string api_key = \"\";\n",
        "\n",
        "    // Make a REST call.\n",
        "    v0::Value response_json = GENC_TRY(Post(api_key, endpoint, input_json));\n",
        "\n",
        "    // Extract text out of JSON\n",
        "    return GeminiParser::GetTopCandidateAsText(response_json);\n",
        "  };\n",
        "  return absl::OkStatus();\n",
        "}\n",
        "```"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GLdSQKb3GrRX"
      },
      "source": [
        "Once the above is defined, a call like the one shown below is used during the\n",
        "runtime construction to plug it into your custom runtime.\n",
        "\n",
        "```c++\n",
        "// Set model inference for Gemini backends.\n",
        "GENC_TRY(GoogleAI::SetInferenceMap(config.model_inference_with_config_map));\n",
        "```"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "F4DOV-RGG92r"
      },
      "source": [
        "### Try it\n",
        "\n",
        "Notice here the model call becomes much cleaner, it's text-in-text-out. There's no JSON formatting and output parsing involved.\n",
        "\n",
        "With this cleaner approach, you can enjoy the benefits of composability with\n",
        "other building blocks. Also, if you have multiple model backends setup the same\n",
        "way, you can easily swap between model backends and leave the rest of the application logic untouched."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "nuQI7kfBHTVM"
      },
      "outputs": [],
      "source": [
        "model_config = genc.authoring.create_rest_model_config(\n",
        "    \"https://generativelanguage.googleapis.com/v1beta/models/gemini-pro:generateContent\",\n",
        "    \"\u003cyour api key\u003e\",\n",
        ")\n",
        "model_call = genc.authoring.create_model_with_config(\n",
        "    \"/cloud/gemini\", model_config\n",
        ")\n",
        "comp = runtime.Runner(comp_pb=model_call)\n",
        "print(comp(\"Tell me a short story\"))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yH0b5l4tZqAt"
      },
      "source": [
        "## Putting it all together\n",
        "\n",
        "Now that you've seen three different types of customizations, as promised,\n",
        "here's a complete example of how it all fits together in a new runtime\n",
        "constructor that you can use to power your own applications. The main function\n",
        "of interest in the call to\n",
        "`CreateLocalExecutor(intrinsics::CreateCompleteHandlerSet(config))` where the\n",
        "runtime construction actually happens. All the code before that sets up the\n",
        "custom `config`, as discussed above, to include the customizations you need.\n",
        "\n",
        "You may also review examples of the included runtime constuctors, like those\n",
        "you've used. With the explanations above, the code should now be easier to\n",
        "understand.\n",
        "\n",
        "Now, the customization in GenC can run much deeper. For more advances topics,\n",
        "consult the extensibility API documentation in\n",
        "[api.md](https://github.com/google/generative_computing/tree/master/generative_computing/docs/api.md)\n",
        "and runtime documentation in\n",
        "[runtime.md](https://github.com/google/generative_computing/tree/master/generative_computing/docs/runtime.md)."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7nUKUXTqpGFe"
      },
      "source": [
        "```c++\n",
        "absl::StatusOr\u003cstd::shared_ptr\u003cExecutor\u003e\u003e CreateDefaultExecutor() {\n",
        "  // This is where you can wire in all your components for the runtime.\n",
        "\n",
        "  intrinsics::HandlerSetConfig config;\n",
        "\n",
        "  // Register custom functions\n",
        "  GENC_TRY(GeminiParser::SetCustomFunctions(config.custom_function_map));\n",
        "\n",
        "  // Register your custom operator\n",
        "  config.custom_intrinsics_list.push_back(new intrinsics::WolframAlpha());\n",
        "\n",
        "  // Register your custom model backend\n",
        "  GENC_TRY(GoogleAI::SetInferenceMap(config.model_inference_with_config_map));\n",
        "\n",
        "  return CreateLocalExecutor(intrinsics::CreateCompleteHandlerSet(config));\n",
        "}\n",
        "```"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "s0ZiAvCHqGGx"
      },
      "source": [
        "### Don't forget to make it available in Python\n",
        "\n",
        "Before we depart, it's worth noting that runtimes you define will often be used\n",
        "in languages other than C++, and as such, it's worth lifting the code to Python\n",
        "or Java. You've seen examples of usage of the default runtime constructor in\n",
        "the tutorials (`create_default_executor`). You can make yours available in the\n",
        "same way, e.g., in Python, by defining an appropriate bindings file, as shown\n",
        "below (and in the example\n",
        "[executor_bindings.cc](https://github.com/google/generative_computing/tree/master/generative_computing/cc/examples/executors/executor_bindings.cc)).\n",
        "\n",
        "```c++\n",
        "// Executor construction methods.\n",
        "m.def(\"create_default_executor\", \u0026CreateDefaultExecutor,\n",
        "      \"Creates a defaul executor with predefined components.\");\n",
        "```"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8h0iaFaktREP"
      },
      "source": [
        "## Next tutorial: buidling modular agents!\n",
        "\n",
        "Congratualtions! You just made enough modular components to build a more interesting LLM agents. In the next tutorial, we'll see in action how all these building blocks will come together to create an LLM agent."
      ]
    }
  ],
  "metadata": {
    "colab": {
      "private_outputs": true,
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
