{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Goy6i-1KOaAY"
      },
      "source": [
        "# Simple chain in LangChain powered by a device-to-Cloud model cascade\n",
        "\n",
        "This tutorial shows how to define simple application logic in LangChain, use our\n",
        "interop APIs to configure it to be powered by a cascade of models that spans\n",
        "across a model in Cloud and an on-device model on Android, and deploy it in a\n",
        "Java app on an Android phone. This illustrates many of the key interoperability\n",
        "and portability benefits of GenC in one concise package. See the follow-up\n",
        "tutorials listed in the parent directory for how you can further extend and\n",
        "customize such logic to power more complex use cases."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Cr8gU1bxnMjA"
      },
      "source": [
        "## Initial setup\n",
        "\n",
        "Before we begin, we need to setup your environment, such that you can continue\n",
        "with the rest of this tutorial undisrupted.\n",
        "\n",
        "*   First, you need to start a Jupyter notebook with the GenC dependency\n",
        "    wired-in, and connect to that notebook - see [SETUP.md](../../../SETUP.md)\n",
        "    at the root of the repo, and the supporting files in the\n",
        "    [Jupyter setup directory](jupyter_setup/) for instructions how to setup the\n",
        "    build and run environment and get Jupyter up and running.\n",
        "\n",
        "*   Next, you need to setup access to the Gemini Pro model that will be used\n",
        "    in the tutorials. Please see the\n",
        "    [instructions](https://ai.google.dev/tutorials/rest_quickstart)\n",
        "    on how to get an API key to access this model through Google AI Studio.\n",
        "\n",
        "*   Finally, for the last portion of the tutorial that includes deployment on\n",
        "    Android, you will need to download Gemma model for GPU (TODO link) and copy\n",
        "    the .tflite model file to your Android phone. Make sure to set the same\n",
        "    path in MODEL_PATH in steps below as the path you choose to copy the model\n",
        "    file to.\n",
        "    ```\n",
        "    adb push {{src-model-dir}}/model_gpu.tflite /data/local/tmp/llm/model_gpu.tflite\n",
        "    ```\n",
        "    Please see instructions in (TODO link) to convert and download supported\n",
        "    models on device.\n",
        "\n",
        "TODO: external link for the on-device setup portion for the Gemma model"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "A67WrPezqN2z"
      },
      "source": [
        "Now, to verify that GenC dependencies are loaded correctly, let's run a bunch\n",
        "of imports we're going to use later."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "tpMQqnD3qKIz"
      },
      "outputs": [],
      "source": [
        "import generative_computing.python as genc\n",
        "from generative_computing.python import authoring\n",
        "from generative_computing.python import interop\n",
        "from generative_computing.python import runtime\n",
        "from generative_computing.python.examples import executor"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "z6Q-Cp9gorfw"
      },
      "source": [
        "## Defining application logic in LangChain\n",
        "\n",
        "We're going to create here an example application logic using LangChain APIs.\n",
        "For the sake of simplicy, let's go with a simple chain that consists of a prompt\n",
        "template feeding into an LLM call. Let's define it as a function, so that we can\n",
        "later play with different models."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "MOihL6fbpTE8"
      },
      "outputs": [],
      "source": [
        "import langchain\n",
        "from langchain.prompts import PromptTemplate\n",
        "\n",
        "def create_my_chain(llm):\n",
        "  return langchain.chains.LLMChain(\n",
        "      llm=llm,\n",
        "      prompt=PromptTemplate(\n",
        "          input_variables=[\"topic\"],\n",
        "          template=\"Tell me about {topic}?\",\n",
        "      ),\n",
        "  )"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "e4Sr0EXyrEwf"
      },
      "source": [
        "## Declaring a model you will use to power the chain\n",
        "\n",
        "Now, let's define a model we can use. In GenC, we refer to models symbolically\n",
        "since the same model may be provisioned differently depending on where you run\n",
        "the code (recall that we want to demonstrate in this tutorial is running your\n",
        "application logic in colab first, but then porting it to run on a phone).\n",
        "To facilitate this, GenC provides interop APIs that enable you to declare the\n",
        "use of a model, e.g., as shown below. For this tutorial, we're going to use\n",
        "the Gemini Pro model from Google Studio AI.\n",
        "\n",
        "Note: Please make sure you have an API_KEY to use as covered in the [Initial Setup section](#scrollTo=Cr8gU1bxnMjA\u0026line=9\u0026uniqifier=1) above."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "pPTN8y8gHSvh"
      },
      "outputs": [],
      "source": [
        "API_KEY = \"\"  #@param\n",
        "\n",
        "my_cloud_model = genc.interop.langchain.CustomModel(\n",
        "    uri=\"/cloud/gemini\",\n",
        "    config=genc.interop.gemini.create_config(API_KEY))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "beCPCSZCR-JO"
      },
      "source": [
        "Now, you can construct the chain with it:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "IlPZJSaNSTtr"
      },
      "outputs": [],
      "source": [
        "my_chain = create_my_chain(my_cloud_model)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "W4M8y-PYTbzE"
      },
      "source": [
        "## Generating portable intermediate representation (IR)\n",
        "\n",
        "Now that you have the application logic (the chain you defined above), we need\n",
        "to translate it into what we call a *portable intermediate representation* (IR\n",
        "for short) that can be deployed on an Android phone. You do this by calling the\n",
        "converstion function provided by GenC, as follows:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "jfxHpZ6Qsqcg"
      },
      "outputs": [],
      "source": [
        "my_portable_ir = genc.interop.langchain.create_computation(my_chain)\n",
        "print(my_portable_ir)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RRszCjdZTwqT"
      },
      "source": [
        "At the time of this writing, this converter only supports a subset of LangChain\n",
        "functionality; we'll work to augment the coverage over time (and we welcome your\n",
        "help if there's a feature of LangChain you'd like to see covered and are willing\n",
        "to contribute it to the platform)."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_5sTduPZUHyt"
      },
      "source": [
        "## Testing the IR locally in the Colab environment\n",
        "\n",
        "Before we move over to deployment on Android, let's first test that the IR is\n",
        "indeed working. While our goal is to run it on-device, we can just as well run\n",
        "it here, in the colab environment (remember, all the code is portable). To do\n",
        "this, we first need to construct a runtime instance:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "FgImBVLEUcGg"
      },
      "outputs": [],
      "source": [
        "my_runtime = genc.examples.executor.create_default_executor()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sD3KJhT_Udhc"
      },
      "source": [
        "Now, the constructor above is provided for convenience in running the examples\n",
        "and tutorials, and is configured with a number of runtime capabilities that we\n",
        "use in this context. Runtimes in GenC are fully modular and configurable, and\n",
        "in most advanced uses, you'll want to configure a runtime that suits the\n",
        "specific environment you want to run in, or your particular application (e.g.,\n",
        "with additional custom dependencies, or without certain dependencies you don't\n",
        "want in your environment). One of the tutorials later in the sequence explains\n",
        "how to do that. For now, the default example runtime will suffice."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4RUATRQOVJq5"
      },
      "source": [
        "Given the runtime and the portable IR we want to run, we can construct a\n",
        "*runner* object that will act like an ordinary Python function, and can\n",
        "be directly invoked, like this:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "KwFnhO-ns3wt"
      },
      "outputs": [],
      "source": [
        "from generative_computing.python import runtime\n",
        "my_runner = genc.runtime.Runner(my_portable_ir, my_runtime)\n",
        "\n",
        "my_runner(\"scuba diving\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tYTmy5Y7Vdvz"
      },
      "source": [
        "## Saving the IR to a file for deployment to phone\n",
        "\n",
        "Now that you tested the IR locally, it's time to deploy it on your phone and\n",
        "test it there. First, let's save the IR into a file on the local filesystem."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "njam4ZuNoiJ6"
      },
      "outputs": [],
      "source": [
        "with open(\"/tmp/genc_demo.pb\", \"wb\") as f:\n",
        "  f.write(my_portable_ir.SerializeToString())"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GhLV1BlXt0fK"
      },
      "source": [
        "## Install Generative Computing Demo app, deploy the IR file on the phone\n",
        "\n",
        "1. First, let's build and install the Generative Computing Demo app.\n",
        "  * TODO: Finalize Docker vs non-Docker setup and pre-built APK or APK build and install\n",
        "\n",
        "\n",
        "2. Next, let's copy the IR file to Android phone. Run following command to copy\n",
        "the file via ADB.\n",
        "  * ```adb push /tmp/genc_demo.pb /data/local/tmp/genc_demo.pb```\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DtmVl-WHtF_T"
      },
      "source": [
        "## Run the demo on phone\n",
        "\n",
        "Make sure your device is connected to internet.\n",
        "\n",
        "1. Open the “Generative Computing Demo” app and type a topic in the UI. As a reminder, here is the prompt template we are using: \"Tell me about {topic}?\"\n",
        "  * Example text to enter in UI: \"scuba diving\"\n",
        "\n",
        "2. See the result. This is the result coming from the Cloud model (make sure\n",
        "that the internet connection is available)."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pfl2tlc8Qqol"
      },
      "source": [
        "## Adding on-device model\n",
        "\n",
        "Now, recall that what we promised to demonstrate in this tutorial running your\n",
        "application logic on a phone, where it might be powered by an on-device LLM\n",
        "while the the phone may be offline. To achieve this, we're going to need to\n",
        "modify the IR to include the on-device model. First, let's declare the use of\n",
        "an on-device model in LangChain, similarly to how we did above for the cloud\n",
        "model.\n",
        "\n",
        "NOTE: Make sure you have downloaded the on-device model on Android at MODEL_PATH as covered in the [initial setup section](#scrollTo=Cr8gU1bxnMjA\u0026line=9\u0026uniqifier=1) above."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-zADdAO2RnKp"
      },
      "outputs": [],
      "source": [
        "MODEL_PATH = \"/data/local/tmp/llm/model_gpu.tflite\"  #@param\n",
        "\n",
        "my_on_device_model = genc.interop.langchain.CustomModel(\n",
        "    uri=\"/device/llm_inference\",\n",
        "    config={\"model_path\": MODEL_PATH})"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_LTw3G7FSGie"
      },
      "source": [
        "Now, we're going to combine the cloud and on-device models into a simple type\n",
        "of moel cascade that spans across cloud and on-device LLMs. For simplicity's\n",
        "sake, let's define a two-model cascade that first tries to hit a cloud backend\n",
        "in case we're online, and that defaults to the use of an on-device model when offline."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "gxc_7cG9Sj2s"
      },
      "outputs": [],
      "source": [
        "my_model_cascade = genc.interop.langchain.ModelCascade(models=[\n",
        "    my_cloud_model, my_on_device_model])\n",
        "\n",
        "my_chain = create_my_chain(my_model_cascade)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sLVUdrTVTAcg"
      },
      "source": [
        "You could've chosen to order models in the cascade differently to achieve a\n",
        "different behavior. Everything is customizable! In the next tutorial in the\n",
        "sequence, we'll show you how you can construct an even more powerful routing\n",
        "mechanism, where routing is based on query sensitivity. For now, this simple\n",
        "cascade will suffice."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_R7mNVfnuBNB"
      },
      "source": [
        "Now, all that we need to do is to re-generate the IR, once again save it to a\n",
        "file, and load it on the phone as shown above."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "pt1Tsi8LuNPN"
      },
      "outputs": [],
      "source": [
        "my_portable_ir = genc.interop.langchain.create_computation(my_chain)\n",
        "with open(\"/tmp/genc_demo.pb\", \"wb\") as f:\n",
        "  f.write(my_portable_ir.SerializeToString())"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7iLVUCbRaLMD"
      },
      "source": [
        "Once you have the IR on the phone, make sure to re-run the demo app so that it\n",
        "loads the updated IR. This time, play with setting the Airplane Mode on your\n",
        "Android phone to On and Off, retry the query, and notice how the result changes\n",
        "depending on the model in use (responses from the on-device and cloud models\n",
        "tend to look differently). Under the hood, the cascade you defined prompts GenC\n",
        "to first try the cloud model, but if it fails (while in airplane mode), it\n",
        "simply falls back to the on-device model (if present).\n",
        "\n",
        "TODO: Add a recorded demo link?"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "last_runtime": {
        "build_target": "",
        "kind": "local"
      },
      "private_outputs": true,
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
