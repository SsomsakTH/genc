# Generative Computing (GenC) APIs

## Overview

The API surface offered by GenC consists of roughly three parts, each targeting
a somewhat different audience, discussed in separate sections below:

*   **Authoring APIs** facilitate construction of *intermediate representation*
    (*IR* for short) i.e., instances of the
    [`computation.proto`](../../proto/v0/computation.proto)'s, by the
    application developers to express the application logic that they want to
    run using GenC in a portable platform- and language-independent manner.

*   **Runtime APIs** facilitate execution of the IR that was created using the
    authoring APIs, in various application contexts. This enables application
    developers to embed the authored IR within their applications (e.g., as an
    ordinary Java callable object in a Java app on Android).

*   **Extensibility APIs** enable power users and platform partners to customize
    the platform for advanced use cases (e.g., connect to custom backends, add
    custom types of operators, etc.), and to contribute new capabilities to
    support new types of frontends, backends, etc., for other customers to use.

## Authoring APIs

There are two kinds of authoring APIs: *interoperability* APIs that enable GenC
to consume code written using another SDK (e.g., LangChain), and lower-level
*native* APIs that the above are layered upon, and that enable advanced uses or
that can be used to add support for new types of frontend SDKs.

### Interop authoring APIs

These authoring APIs enable the developer to define their application logic
using their preferred external SDK, and then have that logic translated into the
IR form with a one-liner call. By convention, the translation call is always
named `genc.interop.XYZ.create_computation(...)` in Python (or in an equivalent
manner in other languages), where `XYZ` is the name of the SDK from which to
convert, the argument represents an object constructed using that SDK, and the
result is a well-formed GenC IR.

In addition, interop APIs contain various extensions that are intended to
augment the SDKs with additional functionality, typically as subclasses that
implement interfaces defined by those SDKs.

At this time, the only supported SDK is LangChain. Over time, we aim to grow the
set of supported SDKs with the help of the 3P/OSS community.

#### Interop authoring APIs for LangChain

These can be found in the
[python/interop/langchain](../../python/interop/langchain/) directory.

Currently included:

*   The converter call `genc.interop.langchain.create_computation` (see above)
    that, at the time of this writing, understands models, prompt templates,
    chains, agents, and custom extensions listed below (with more to be added
    over time).

*   Extensions for custom models, model cascades, chains, agents, and tools that
    implement the base interfaces defined in LangChain.

The snippet below illustrates the use of several of the above:

```
my_chain = langchain.chains.LLMChain(
    llm=genc.interop.langchain.CustomModel(uri="/cloud/gemini"),
    prompt=langchain.prompts.PromptTemplate(
        input_variables=["location"],
        template="Q: What should I pack for a trip to {location}? A: ",
    ),
)

portable_ir = interop.langchain.create_computation(my_chain)
```

### Native authoring APIs

These APIs enable the developer to directly construct the IR, and for the most
part, they map in a roughly one-to-one manner to the IR constructs. In addition
to APIs that map 1:1 to the IR constructs, we offer a handful of convenience
APIs layered on top of those to support writing more compact code. Over time, we
anticipate these to evolve into separate higher-level API layers. As noted
above, the primary audience for these APIs is framework and platform partners
who want to implement interop APIs for new types of SDKs, and power users who
need direct control and customizability, e.g., to complement code written in an
external SDK with a segment that can't be expressed in that SDK due to its
limitations (but can be coded directly using a native authoring API and merged
with the IR generated by the converter).

The authoring APIs are currently offered in C++ and in Python, with Java and
other languages coming later. The API is consistent across all languages, with
C++ offering the most complete coverage, and other languages in sync via the
appropriate language-specific bindings (such as `pybind11`, `JNI`, etc.).

**C++ native authoring APIs** can be found in
[cc/authoring](../../cc/authoring/), with all the basic IR constructs defined
in [constructor.h](../../cc/authoring/constructor.h). Longer descriptions of
these basic primitives can be found in
[cc/intrinsics/intrinsic_uris.h](../../cc/intrinsics/intrinsic_uris.h). Note
that this system of operators is extensible. Deployments using GenC can add new
operators targeting their specific problem domain that aren't listed or
documented here. All functions in the authoring API take and return pieces of
the IR, and are named `CreateXXX` after the element of the IR they are designed
to construct.

The snippet below illustrates the use of the C++ authoring to expressed an agent
reasoning example (you can find the full working code in
[cc/examples/math_tool_agent.cc](../../cc/examples/math_tool_agent.cc)):

```
// Create modular computations, orders doesn't matter, these are just protos.
// We employ lazy execution here. (eager is possible too).
// Logger prints to terminal
v0::Value log_it = GENC_TRY(CreateLogger());

// Context keeps the interaction history with model.
v0::Value read_context = GENC_TRY(CreateCustomFunction("/local_cache/read"));
v0::Value add_to_context =
    GENC_TRY(CreateCustomFunction("/local_cache/write"));

// Create a custom model call by utilizing Gemini behind REST endpoint.
std::string api_key = absl::GetFlag(FLAGS_api_key);
std::string end_point =
    "https://generativelanguage.googleapis.com/v1beta/models/"
    "gemini-pro:generateContent?key=" +
    api_key;
v0::Value rest_call = GENC_TRY(CreateRestCall(end_point));
v0::Value str_to_json_request =
    GENC_TRY(CreateCustomFunction("/gemini_parser/wrap_text_as_input_json"));
v0::Value extrat_top_candidate = GENC_TRY(
    CreateCustomFunction("/gemini_parser/get_top_candidate_as_text"));
SmartChain model_call =
    SmartChain() | str_to_json_request | rest_call | extrat_top_candidate;

// We use WolframAlpha as a Tool to solve simple math questions.
std::string appid = absl::GetFlag(FLAGS_appid);
v0::Value extract_result_from_wolfram = GENC_TRY(CreateInjaTemplate(
    "{% if queryresult.pods %}{{queryresult.pods.0.subpods.0.plaintext}}{% "
    "endif %}"));
SmartChain solve_math =
    SmartChain() | CreateWolframAlpha(appid) | extract_result_from_wolfram;

SmartChain reasoning_loop =
    SmartChain() | read_context | model_call |
    CreateCustomFunction("/react/parse_thought_action") | log_it |
    CreateRegexPartialMatch("Finish") | add_to_context |
    CreateCustomFunction("/react/extract_math_question") | solve_math |
    CreateCustomFunction("/react/format_observation") | log_it |
    add_to_context;

// Set the max number of iterations of a reasoning loop.
// This utilizes a RepeatedConditionalChain under the hood, which allows a
// chain to be repeatedly executed until a break condition is met.
reasoning_loop.SetNumIteration(8);

SmartChain step_by_step_math_agent =
    SmartChain() | CreatePromptTemplate(kInstructionTemplate) |
    add_to_context | log_it | reasoning_loop;

v0::Value computation = GENC_TRY(step_by_step_math_agent.Build());
```

**Python native authoring APIs** are lifted from C++ via
[constructor_bindings.cc](../../cc/authoring/constructor_bindings.cc), and can
be found re-exported with additional documentation in
[python/authoring/constructors.py](../../python/authoring/constructors.py), and
with names, types of arguments and results closely matching those from C++ (but
adapted for use in Python).

In addition, as an added convenience in constructing more complex IR logic,
there's experimental support for defining IR via decorated Python functions that
are traced; see
[python/examples/authoring_demo.py](../../python/examples/authoring_demo.py) or
the snippet below for an example of what this looks like.

```
@genc.authoring.traced_computation
def comp(x):
    template_str = "Q: What should I pack for a trip to {location}? A: "
    prompt = genc.authoring.prompt_template[template_str](x)
    model_output = genc.authoring.model_inference["test_model"](prompt)
    return model_output
```

This form of authoring is currently under development.

**Java native authoring APIs** currently don't exist, but will be added in a
manner that mirrors those in Python, by virtue of lifting C++ authoring APIs
via JNI and adapting them for usage from within Java.

## Runtime APIs

The runtime API picks up where the authoring API left off, by taking IR, and
translating it into a callable object that can be used like a regular function.
This API is organized around two key concepts:

* **Executor** is a GenC system component that's responsible for execution of
  the IR, taking to various backends, potentially interacting with executors
  on other machines, etc. An application that uses GenC will typically setup
  an executor once, and then reuse it for all execution afterwards. Executors
  are not associated with any concrete piece of IR.

* **Runner** is a callable object that wraps around a concrete piece of IR and
  a concrete executor, and handles all aspects of execution, possibly including
  translation between Java or Python arguments and results and internal GenC
  data structures (with the exception of C++, where no such translation is done
  as we presume that customers using C++ prefer to operate at a lower level).

The typical flow at runtime looks as follows:

* Construct an executor instance at the beginning, e.g., while initializing the
application.
* Load one or more pieces of IR, and construct the runners for each.
* Use these runners afterwards as a regular callables.

Note that in GenC, runtime itself is modular, and thus constructing executors
is also done via GenC APIs; those are discussed in the extensibility section
below. In order to facilitate simple developer experience for first-time users,
we provide a handle of sample runtime constructors for various environments
that can be constructed with one-line calls, and come with a set of dependencies
sufficient to run examples in all the tutorials. For more advances uses, one
may wish to setup their own runtime executors; see below for how to do this.

The remaining details are language-specific, albeit we strive for consistency;
see the following subsections for further details and examples of usage.

### Python runtime API

A version of a runner for Python (e.g., for use in Colab) is defined in
[python/runtime/runner.py](../../python/runtime/runner.py). It takes IR as the
first, and executor as the second optional argument. A one-liner executor
constructor that can be used for all the tutorials and examples is provided in
[TODO]().

TODO add a proper default Python executor constructor above and below

Here's a code snippet that illustrates example usage:

```
executor = ...
portable_ir = genc.authoring.create_model('test_model')
runner = genc.runtime.Runner(portable_ir, executor)
python_result = runner('Boo!')
```

### Java runtime API

A version of a runner for Java (e.g., for use on Android) is defined in
[java/src/java/org/generativecomputing/Runner.java]
(../../java/src/java/org/generativecomputing/Runner.java). Similarly to its
Python counterpart, its constructor takes IR and an executor. A one-liner
executor constructor that can be used for the tutorials and examples to run
on Android is provided in
[java/src/java/org/generativecomputing/examples/apps/gencdemo/DefaultAndroidExecutor.java]
(../../java/src/java/org/generativecomputing/examples/apps/gencdemo/DefaultAndroidExecutor.java). As noted above, you can setup a custom executor
using extensibility APIs discussed below.

Here's a code snippet that illustrates example usage:

```
executor = new DefaultAndroidExecutor(getApplicationContext());
InputStream stream = new FileInputStream("/data/local/tmp/portable_ir.pb");
Value portable_ir = Value.parseFrom(stream, getExtensionRegistry());
runner = Runner.create(portable_ir, executor.getExecutorHandle());
...
javaResult = runner.call(javaArgument);
```

### C++ runtime API

TODO add a proper default C++ executor constructor above and below

The C++ version of a runner is defined in
[cc/runtime/runner.h](../../cc/runtime/runner.h). Unlike its Python and Java
counterparts, this runner requires the caller to provide arguments and consume
results in the form of GenC protos.

Here's a code snippet that illustrates example usage:

```
std::shared_ptr<Executor> executor = GENC_TRY(CreateDefaultExecutor());
v0::Value portable_ir = ...
Runner runner = GENC_TRY(Runner::Create(portable_ir, executor));
v0::Value arg;
arg.set_str("Boo!");
result = runner.Run(portable_ir, arg);
```

## Extensibility APIs

As noted above, GenC runtime itself is modular and configurable, and a power
user may want to use the APIs described in this section to configure it to suit
their needs (as opposed to using the example one-liner constructors as in the
tutorials or some of the earlier examples listed above).

The extensibility surface spoans several layers, and is organized around the
following key concepts:

* **Executors** are the heart of the runtime, and are responsible for executing
  the IR. In principle, an executor is anything that implements
  [cc/runtime/executor.h](../../cc/runtime/executor.h). Even for a customized
  runtime, executors are generally constructed by a `CreateLocalExecutor` call
  defined in [cc/runtime/executor_stacks.h](../../cc/runtime/executor_stacks.h);
  the argument to this call is a set of *handlers* (see below). If desired, one
  may provide their own implementation of an executor, but in most cases, we
  don't expect this to be necessary (except if planning, e.g., to interoperate
  with legacy runtime infrastructure, etc.).

* **Handlers** are pluggable modules responsible for implementing specific types
  of operators that may appear in the IR, such as model inference calls, custom
  function calls, control flow abstractions, etc. One can setup the runtime with
  an arbitrary set of operators, including their own custom operators, or with
  some operators removed to simplify runtime dependencies. This is achieved by
  passing an appropriate set of handlers. In most situations, a set of handlers
  will be created by a call `CreateCompleteHandlerSet` defined in
  [cc/intrinsics/handler_sets.h](../../cc/intrinsics/handler_sets.h); the
  argument to this call is a structure that contains customizable configuration
  for the commonly used types of handlers. The call above creates a set of
  handlers for all known operators included in the GenC codebase. If you want to
  add your own handlers, you can simply include them in the handler_set_config
  in the `custom_intrinsics_list`. If you want to create a handler set with only
  certain handlers included (e.g., for a more streamlined runtime), this is
  also quite easy - simply see how this is achieved in
  [cc/intrinsics/handler_sets.cc](../../cc/intrinsics/handler_sets.cc).

* **Inference map** is one of the fields in the handler config that configures
  the processing of `model_inference` calls. It's a map in which keys are the
  model names (those that can appear in the IR), and values are C++ callables
  that take a model prompt at input and return the result of the model inference
  call at output. You will want to customize the inference map when setting up
  custom model backends.

* **Custom function map** is likewise a mapping from custom function names that
  may appear in the IR to C++ handlers that contain their implementations. The
  same custom function can be implemented differently depending on the
  environment. Implementations of commonly used custom functions are included
  in the [cc/modules](../../cc/modules/) directory.

* **Delegate map** is a mapping from the names of runtime environments to the
  C++ callables that delegate processing to those environments. This may be set,
  e.g., in an on-device executor to delegate entire segments of IR to run in
  cloud. You will not need to set this if you're running locally.

Here's an example snippet of C++ code from
[cc/runtime/android/android_executor_stacks.cc]
(../../cc/runtime/android/android_executor_stacks.cc)
that puts these concepts in action. Note that we use all the runtime constructor
calls mentioned above (`CreateCompleteHandlerSet` and `CreateLocalExecutor`),
and the only thing that's being customized is the handler config. The two calls
below simply register handlers for Open AI and Gemini models in the config to
enable routing those calls from IR when deployed on Android.

```
absl::StatusOr<std::shared_ptr<Executor>> CreateAndroidExecutor(
    JavaVM* jvm, jobject open_ai_client, jobject google_ai_client) {
  intrinsics::HandlerSetConfig config;
  SetOpenAiModelInferenceHandler(
      &config, jvm, open_ai_client, kOpenAIModelUri);
  SetGoogleAiModelInferenceHandler(
      &config, jvm, google_ai_client, kGeminiModelUri);
  return CreateLocalExecutor(intrinsics::CreateCompleteHandlerSet(config));
}
```

Now, as noted above, GenC enables you to add your own custom operators to the
system. Before you do that, you may want to review the list of operators that
already exist. Those are documented in
[cc/intrinsics/intrinsic_uris.h](../../cc/intrinsics/intrinsic_uris.h), and
their implementations are contained in the same directory (in
[cc/intrinsics](../../cc/intrinsics/)).

Adding a new operator requires simply implementing one of two handler interfces
defined in
[cc/runtime/intrinsic_handler.h](../../cc/runtime/intrinsic_handler.h), either
by deriving from the `InlineIntrinsicHandlerBase` for operators that simply
want to process data in-and-out, or from `ControlFlowIntrinsicHandlerBase` for
custom control flow and other general type of operators.

The former type (i.e., *inline*) operators simply take and return the `Value`
proto (defined in
[proto/v0/computation.proto](../../proto/v0/computation.proto)). This is the
same proto used to define the IR. In addition, the handler that implements it
has access to the piece of IR where the operator is being used, and that may
contain an optional static parameter.

Below is an example snippet of code for the logical not operator:

```
absl::Status LogicalNot::ExecuteCall(
    const v0::Intrinsic& intrinsic_pb,
    const v0::Value& arg,
    v0::Value* result) const {
  if (!arg.has_boolean()) {
    return absl::InvalidArgumentError("Argument does not contain boolean.");
  }
  result->set_boolean(!arg.boolean());
  return absl::OkStatus();
}
```

The latter type (i.e., *control flow*) operators can perform arbitrary types of
processing. These operators are given access to the full executor interface (as
defined in [cc/runtime/executor.h](../../cc/runtime/executor.h)) for the
executor in which they've been installed as *context*. This is more complex to
use compared to working with protos that come in-and-out, but it provides an
unrestricted ability to orchestrate processing from within the body of the
custom operator (see [cc/runtime/executor.h](../../cc/runtime/executor.h) for
the type of calls these operators can make).

Below is an example snippet of code for the conditional operator:

```
absl::StatusOr<ControlFlowIntrinsicHandlerInterface::ValueRef>
Conditional::ExecuteCall(
    const v0::Intrinsic& intrinsic_pb,
    std::optional<ValueRef> arg,
    Context* context) const {
  if (!arg.has_value()) {
    return absl::InvalidArgumentError("Missing condition.");
  }
  v0::Value cond_pb;
  GENC_TRY(context->Materialize(arg.value(), &cond_pb));
  if (!cond_pb.has_boolean()) {
    return absl::InvalidArgumentError(
        absl::StrCat("Condition is not a Boolean: ", cond_pb.DebugString()));
  }
  return context->CreateValue(cond_pb.boolean()
      ? intrinsic_pb.static_parameter().struct_().element(0)
      : intrinsic_pb.static_parameter().struct_().element(1));
}
```

Similarly to the authoring surface, extensibility APIs for languages other than
C++ are provided by lifting the C++ API via `pybind11` and `JNI``.
