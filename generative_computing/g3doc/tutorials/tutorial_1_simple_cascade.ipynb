{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Goy6i-1KOaAY"
      },
      "source": [
        "# Simple chain in LangChain powered by a device-to-Cloud model cascade\n",
        "\n",
        "This tutorial shows how to define simple application logic in LangChain, use our\n",
        "interop APIs to configure it to be powered by a cascade of models that spans\n",
        "across a model in Cloud and an on-device model on Android, and deploy it in a\n",
        "Java app on an Android phone. This illustrates many of the key interoperability\n",
        "and portability benefits of GenC in one concise package. See the follow-up\n",
        "tutorials listed in the parent directory for how you can further extend and\n",
        "customize such logic to power more complex use cases."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Cr8gU1bxnMjA"
      },
      "source": [
        "## Initial setup\n",
        "\n",
        "Before we begin, we need to setup your environment, such that you can continue\n",
        "with the rest of this tutorial undisrupted.\n",
        "\n",
        "TODO: add all the setup steps here, including things like:\n",
        "* On-device model setup:\n",
        "   * Please see instructions (TODO: link MediaPipe announcement and developer guide when published) to convert and download supported models on device.\n",
        "   * For this tutorial, download Gemma model for GPU (TODO: external link) and copy the .tflite model file to your Android phone. Make sure to set the same path in MODEL_PATH in steps below as the path you choose to copy the model file to.\n",
        "     *  adb push {{src-model-dir}}/model_gpu.tflite /data/local/tmp/llm/model_gpu.tflite\n",
        "\n",
        "* starting a jupyter notebook with the GenC dependency wired-in\n",
        "* connecting to that notebook"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "z6Q-Cp9gorfw"
      },
      "source": [
        "## Defining application logic in LangChain\n",
        "\n",
        "We're going to create here an example application logic using LangChain APIs.\n",
        "For the sake of simplicy, let's go with a simple chain that consists of a prompt\n",
        "template feeding into an LLM call. Let's define it as a function, so that we can\n",
        "later play with different models."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "MOihL6fbpTE8"
      },
      "outputs": [],
      "source": [
        "import langchain\n",
        "from langchain.prompts import PromptTemplate\n",
        "\n",
        "def create_my_chain(llm):\n",
        "  return langchain.chains.LLMChain(\n",
        "      llm=llm,\n",
        "      prompt=PromptTemplate(\n",
        "          input_variables=[\"topic\"],\n",
        "          template=\"Tell me about {topic}?\",\n",
        "      ),\n",
        "  )"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pfl2tlc8Qqol"
      },
      "source": [
        "## Declaring models you will use to power the chain\n",
        "\n",
        "Now, recall that what we want to demonstrate in this tutorial is running your\n",
        "application logic on a phone, where it might be powered by an on-device LLM.\n",
        "To facilitate this, GenC provides interop APIs that enable you to declare the\n",
        "use of an on-device model, e.g., like this:\n",
        "\n",
        "Note: Make sure you have downloaded the on-device model on Android at MODEL_PATH as covered in the [initial setup section](#scrollTo=Cr8gU1bxnMjA\u0026line=9\u0026uniqifier=1) above."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-zADdAO2RnKp"
      },
      "outputs": [],
      "source": [
        "import generative_computing as genc\n",
        "\n",
        "MODEL_PATH = \"/data/local/tmp/llm/model_gpu.tflite\"  #@param\n",
        "\n",
        "my_on_device_model = genc.interop.langchain.CustomModel(\n",
        "    uri=\"/device/llm_inference\",\n",
        "    config={\"model_path\": MODEL_PATH})"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "beCPCSZCR-JO"
      },
      "source": [
        "Now, you can construct the chain with it:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "IlPZJSaNSTtr"
      },
      "outputs": [],
      "source": [
        "my_chain = create_my_chain(my_on_device_model)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AC8LWdQ4HLXS"
      },
      "source": [
        "Similarly, let's define a cloud model. We will be using this in the next part of the tutorial.\n",
        "\n",
        "Here, we are using Gemini Pro model from Google Studio AI.\n",
        "\n",
        "Please [see instructions](https://ai.google.dev/tutorials/rest_quickstart) on how to get an API key to access this model:\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "pPTN8y8gHSvh"
      },
      "outputs": [],
      "source": [
        "import generative_computing as genc\n",
        "\n",
        "API_KEY = \"\"  #@param\n",
        "\n",
        "my_cloud_model = genc.interop.langchain.CustomModel(\n",
        "    uri=\"/cloud/gemini\",\n",
        "    config=genc.interop.gemini.create_config(API_KEY))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_LTw3G7FSGie"
      },
      "source": [
        "But, let's make things more interesting. As noted at the outset of the tutorial,\n",
        "we'll want to illustrate the use of a cascade of models that spans across cloud\n",
        "and on-device LLMs. For simplicity's sake, let's define a two-model cascade that\n",
        "first tries to hit a cloud backend (in case we're online), and that defaults to\n",
        "the use of an on-device model otherwise (if offline):"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "gxc_7cG9Sj2s"
      },
      "outputs": [],
      "source": [
        "my_model_cascade = genc.interop.langchain.ModelCascade(models=[\n",
        "    my_cloud_model, my_on_device_model])\n",
        "\n",
        "my_chain = create_my_chain(my_model_cascade)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sLVUdrTVTAcg"
      },
      "source": [
        "You could've chosen to order models in the cascade differently to achieve a\n",
        "different behavior. Everything is customizable! In the next tutorial in the\n",
        "sequence, we'll show you how you can construct an even more powerful routing\n",
        "mechanism, where routing is based on query sensitivity. For now, this simple\n",
        "cascade will suffice."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "W4M8y-PYTbzE"
      },
      "source": [
        "## Generating portable intermediate representation (IR)\n",
        "\n",
        "Now that you have the application logic (the chain you defined above), we need\n",
        "to translate it into what we call a *portable intermediate representation* (IR\n",
        "for short) that can be deployed on an Android phone. You do this by calling the\n",
        "converstion function provided by GenC, as follows:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "jfxHpZ6Qsqcg"
      },
      "outputs": [],
      "source": [
        "my_portable_ir = genc.interop.langchain.create_computation(my_chain)\n",
        "print(my_portable_ir)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RRszCjdZTwqT"
      },
      "source": [
        "At the time of this writing, this converter only supports a subset of LangChain\n",
        "functionality; we'll work to augment the coverage over time (and we welcome your\n",
        "help if there's a feature of LangChain you'd like to see covered and are willing\n",
        "to contribute it to the platform)."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_5sTduPZUHyt"
      },
      "source": [
        "## Testing the IR locally in the Colab environment\n",
        "\n",
        "Before we move over to deployment on Android, let's first test that the IR is\n",
        "indeed working. While our goal is to run it on-device, we can just as well run\n",
        "it here, in the colab environment (remember, all the code is portable). To do\n",
        "this, we first need to construct a runtime instance:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "FgImBVLEUcGg"
      },
      "outputs": [],
      "source": [
        "my_runtime = ..."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-ef7hcXsUfrj"
      },
      "source": [
        "TODO: add the OSS runtime constructor above for Linux envrionments"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sD3KJhT_Udhc"
      },
      "source": [
        "Now, the constructor above is provided for convenience in running the examples\n",
        "and tutorials, and is configured with a number of runtime capabilities that we\n",
        "use in this context. Runtimes in GenC are fully modular and configurable, and\n",
        "in most advanced uses, you'll want to configure a runtime that suits the\n",
        "specific environment you want to run in, or your particular application (e.g.,\n",
        "with additional custom dependencies, or without certain dependencies you don't\n",
        "want in your environment). One of the tutorials later in the sequence explains\n",
        "how to do that. For now, the default example runtime will suffice."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4RUATRQOVJq5"
      },
      "source": [
        "Given the runtime and the portable IR we want to run, we can construct a\n",
        "*runner* object that will act like an ordinary Python function, and can\n",
        "be directly invoked, like this:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "KwFnhO-ns3wt"
      },
      "outputs": [],
      "source": [
        "my_runner = genc.runtime.Runner(my_portable_ir, my_runtime)\n",
        "\n",
        "my_runner(\"scuba diving\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tYTmy5Y7Vdvz"
      },
      "source": [
        "## Saving the IR to a file and deployint it on the phone\n",
        "\n",
        "Now that you tested the IR locally, it's time to deploy it on your phone and\n",
        "test it there. First, let's save the IR into a file on the local filesystem."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "njam4ZuNoiJ6"
      },
      "outputs": [],
      "source": [
        "from google3.pyglib import gfile\n",
        "\n",
        "# saving to a file\n",
        "with gfile.Open(\"/tmp/genc_tutorial_1.pb\", \"wb\") as f:\n",
        "  f.write(my_portable_ir.SerializeToString())\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zjDgv4OkWBWb"
      },
      "source": [
        "TODO: add file saving above that works in OSS"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GhLV1BlXt0fK"
      },
      "source": [
        "TODO: continue with the rest of this tutorial to explain how to load it on the\n",
        "phone, what the code in Java looks like, how to run it there, etc."
      ]
    }
  ],
  "metadata": {
    "colab": {
      "last_runtime": {
        "build_target": "",
        "kind": "local"
      },
      "private_outputs": true,
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
