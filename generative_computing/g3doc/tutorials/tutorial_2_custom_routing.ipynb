{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CUKRoMbYOfWz"
      },
      "source": [
        "# Tutorial 2: Custom Routing\n",
        "\n",
        "Here we use one model to score an input text, then based on the score we route it to one of the two models."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rgiLioAo28Lj"
      },
      "source": [
        "## Initial Setup\n",
        "\n",
        "Please follow [Instructions](.../tutorial_1_simple_cascade.ipynb) in Tutorial 1 to set up on-device model and other\n",
        "runtime configuration."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GE6p3kjYPsGD"
      },
      "source": [
        "## Define a two model routing\n",
        "\n",
        "We use the on-device model to estimate if a query is politically sensitive.\n",
        "\n",
        "* If yes, we use the cloud model gemini-pro to answer the query. This increases\n",
        "the accuracy in answering these questions, while balancing\n",
        "sensitivity against model inference cost.\n",
        "* If not, we use the cheaper on-device gemma model to answer the query.\n",
        "\n",
        "Let's define the on-device and cloud models we will be using in this demo."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4CbNgN_gCBAx"
      },
      "outputs": [],
      "source": [
        "import generative_computing as genc\n",
        "\n",
        "MODEL_PATH = \"/data/local/tmp/llm/model_gpu.tflite\"  #@param\n",
        "\n",
        "my_on_device_model = genc.interop.langchain.CustomModel(\n",
        "    uri=\"/device/llm_inference\",\n",
        "    config={\"model_path\": MODEL_PATH})\n",
        "\n",
        "my_on_device_model_ir = genc.interop.langchain.create_computation(my_on_device_model)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "r1_1B8uqCLd1"
      },
      "outputs": [],
      "source": [
        "import generative_computing as genc\n",
        "\n",
        "API_KEY = \"\"  #@param\n",
        "\n",
        "my_cloud_model = genc.interop.langchain.CustomModel(\n",
        "    uri=\"/cloud/gemini\",\n",
        "    config=genc.interop.gemini.create_config(API_KEY))\n",
        "\n",
        "my_cloud_model_ir = genc.interop.langchain.create_computation(my_cloud_model)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QgfSK2A9kb5m"
      },
      "source": [
        "Now, let's define the scorer model. Here, for demo purposes, a simple few-shot\n",
        "prompt template is used to assess political sensitivity.\n",
        "\n",
        "One can replace this with more advanced scorer models.\n",
        "\n",
        "The scorer uses on-device model."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Q1pc4xN5PD_7"
      },
      "outputs": [],
      "source": [
        "import generative_computing as genc\n",
        "import langchain\n",
        "from langchain.prompts import PromptTemplate\n",
        "\n",
        "prompt_template = \"\"\"\n",
        " Instructions: The following are questions that can touch on sensitive or\n",
        " political topics. Please return True or False boolean with no explanation\n",
        " if the question touches on sensitive or political topic. Q: what are your\n",
        " views on democracy and communism? A: True Q: what are your views on\n",
        " current ruling party in US? A: True Q: What is the weather today? A: False\n",
        " Q: {query} A:\n",
        "\"\"\"\n",
        "\n",
        "scorer_model = langchain.chains.LLMChain(\n",
        "      llm=my_on_device_model,\n",
        "      prompt=PromptTemplate(\n",
        "          input_variables=[\"query\"],\n",
        "          template=prompt_template)\n",
        "      )\n",
        "\n",
        "regex_match = (\n",
        "    genc.authoring.create_regex_partial_match(\"A: True|A: true|true|True\")\n",
        ")\n",
        "\n",
        "scorer_chain = (\n",
        "    genc.interop.langchain.CustomChain()\n",
        "    | scorer_model\n",
        "    | regex_match\n",
        "  )\n",
        "\n",
        "scorer_chain_ir = genc.interop.langchain.create_computation(scorer_chain)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "08ebql2lF8ZG"
      },
      "source": [
        "Next, we define a conditional where if the scorer assesses the query to be\n",
        "sensitive and returns True, we route the query to cloud model, else we route it\n",
        "to on-device model.\n",
        "\n",
        "Additionally, we generate a portable intermediate representation (IR), that we\n",
        "can run on workstation or Android."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "M5FjkSQbGY8h"
      },
      "outputs": [],
      "source": [
        "import generative_computing as genc\n",
        "\n",
        "\n",
        "portable_ir = genc.authoring.create_lambda_from_fn(\n",
        "    \"x\",\n",
        "    lambda arg: genc.authoring.create_conditional(\n",
        "        genc.authoring.create_call(scorer_chain_ir, arg),\n",
        "        genc.authoring.create_call(my_cloud_model_ir, arg),\n",
        "        genc.authoring.create_call(my_on_device_model_ir, arg),\n",
        "    ),\n",
        ")\n",
        "print(portable_ir)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Scf-MMnkPoU9"
      },
      "source": [
        "## Run it"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6kSazHPnQmwt"
      },
      "outputs": [],
      "source": [
        "import generative_computing as genc\n",
        "\n",
        "runner = genc.runtime.Runner(portable_ir,\n",
        "                             genc.examples.executor.create_default_executor())\n",
        "\n",
        "runner(\"What are your views on ice cream?\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cssG2tkPHc4c"
      },
      "source": [
        "## Save the IR to a file\n",
        "\n",
        "Run the following code to generate the a file containing the IR from above\n",
        "computation."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "OY1DlYqnJ9XI"
      },
      "outputs": [],
      "source": [
        "from google3.pyglib import gfile\n",
        "\n",
        "with gfile.Open(\"/tmp/genc_demo.pb\", \"wb\") as f:\n",
        "  f.write(portable_ir.SerializeToString())\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8fk_BJZ6K5V3"
      },
      "source": [
        "## Install Generative Computing Demo app and deploy IR file to phone\n",
        "\n",
        "Please see [\"Install Generative Computing Demo app and deploy the IR file to phone\"](.../tutorial_1_simple_cascade.ipynb) in Tutorial 1 for instructions.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cDbkrAZ9boui"
      },
      "source": [
        "## Run the demo\n",
        "\n",
        "1. Open Generative Computing Demo app, and enter a sensitive query. See response.\n",
        "\n",
        "2. Next, enter a non-sensitive query. See response.\n",
        "\n",
        "3. Interesting observation: Notice the length of the text response for non-sensitive query compared to the sensitive query. Any guesses?\n",
        "  *  The non-sensitive query is routed to on-device model which in our demo setup returns shorter text responses to balance response time against lesser compute resources on device while the cloud model returns more verbose response.\n",
        "\n",
        "4. The intent of this custom routing example is to illustrate that developers can create custom routing policies to dynamically choose which model to use per\n",
        "different use-cases and model capabilities."
      ]
    }
  ],
  "metadata": {
    "colab": {
      "last_runtime": {
        "build_target": "",
        "kind": "local"
      },
      "private_outputs": true,
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
