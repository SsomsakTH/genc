{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8rBGCGFlXf_I"
      },
      "source": [
        "# Tutorial 3: Custom Runtime\n",
        "\n",
        "GenC can be customized for your own product domain. In this tutorial we explore how the following topics:\n",
        "\n",
        "* `CustomFunction` - define arbirary function and make it an operator in GenC, for instance, a parsers of model output can be modeled as custom function.\n",
        "* Domain Specific Tools as `Intrinsics` - define own GenAI buidling blocks, for instance wolfram is a tool.\n",
        "* `ModelInference` - is a GenC predefined intrinsics that allows user defined model calls. It's helpful for you to standardize multiple model backends and easily switch between them.\n",
        "* `ExecutorStack` which will be able to keep global context and bring all building blocks together under one Custom runtime for your use cases.\n",
        "\n",
        "To motivate this reading, in the next tutorial, we'll use all these building blocks to create LLM Agents.\n",
        "\n",
        "note: if you are interested in GenC internal after this tutorial, read more from [extensibility API documentation](api.md)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rBiAszEwZUur"
      },
      "source": [
        "## CustomFunction\n",
        "\n",
        " User defined functions that can be mix-n-matched with native GenC building blocks."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "f7ft3_qUZCBI"
      },
      "source": [
        "## Intrinsics\n",
        "You can consider them as native building blocks of your custom SDK.\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-m6PBZLwsKxx"
      },
      "source": [
        "## ModelInference\n",
        "GenC provides some predefined ways to call popular model backends such as ChatGPT, Gemini. The reality is that you'll wan to customize them for your own use cases.\n",
        "\n",
        "There are many model backends out there. Gemini, ChatGPT, self hosted LLAMA... each comes with a different input and output. Is there a way to bring all of them under a consistent interface, so you can easily chain them with other operators or switch between diffent models? This section provides the solution to this question."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yH0b5l4tZqAt"
      },
      "source": [
        "## ExecutorStack\n",
        "\n",
        "A custom runtime that holds context/state and brings all building blocks together."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8h0iaFaktREP"
      },
      "source": [
        "## Intro to next tutorial: Buidling Modular Agents\n",
        "\n",
        "Congratualtions you just made enough modular components to build a more interesting LLM agents. Next tutorial we'll see in action how all these building blocks will come together to create an LLM agent."
      ]
    }
  ],
  "metadata": {
    "colab": {
      "private_outputs": true,
      "provenance": []
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
