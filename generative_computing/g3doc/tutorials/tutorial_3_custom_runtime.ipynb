{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8rBGCGFlXf_I"
      },
      "source": [
        "# Tutorial 3: Custom Runtime\n",
        "\n",
        "GenC can be customized for your own product domain. In this tutorial we explore how the following topics:\n",
        "\n",
        "* `CustomFunction` - define arbirary function and make it an operator in GenC, for instance, a parsers of model output can be modeled as custom function.\n",
        "* Domain Specific Tools as `Intrinsics` - define own GenAI buidling blocks, for instance wolfram is a tool.\n",
        "* `ModelInference` - is a GenC predefined intrinsics that allows user defined model calls. It's helpful for you to standardize multiple model backends and easily switch between them.\n",
        "* `ExecutorStack` which will be able to keep global context and bring all building blocks together under one Custom runtime for your use cases.\n",
        "\n",
        "To motivate this reading, in the next tutorial, we'll use all these building blocks to create LLM Agents.\n",
        "\n",
        "note: if you are interested in GenC internal after this tutorial, read more from [extensibility API documentation](api.md)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rBiAszEwZUur"
      },
      "source": [
        "## 1. CustomFunction\n",
        "\n",
        "These are user defined functions that can be mix-n-matched with native GenC\n",
        "building blocks.\n",
        "\n",
        "For example, models often takes a structured (such as JSON) input and output.\n",
        "However, in a typical \"chain\", we want the operator that calls the model to be\n",
        "text-in-text-out. This means, there're two helper components in the model call\n",
        "chain:\n",
        "\n",
        "*   Format model input - given text, format it into a JSON request for the model\n",
        "*   Parse model output - give a JSON response, parse it into a text\n",
        "\n",
        "These are inline functions can be executed by itself. Therefore we model it as\n",
        "`CustomFunction`. Let's take a look how this is done."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "f6kaczFq8ZN8"
      },
      "source": [
        "### 1.1 Define the function interface\n",
        "\n",
        "These are plain C++ functions you would write without GenC. There are two steps it would take to make it work with GenC.\n",
        "\n",
        "1. The function signiture is Value-in-Value-out. GenC `Value` is a proto that can carry multimodal data. This would make the function chainable to other operator regardless of your data type.\n",
        "   \n",
        "   * `static absl::StatusOr\u003cv0::Value\u003e GetTopCandidateAsText(v0::Value input);`\n",
        "   * `static absl::StatusOr\u003cv0::Value\u003e WrapTextAsInputJson(v0::Value input);`\n",
        "\n",
        "2. We need to expose these functions to the runtime.\n",
        "  `static absl::Status SetCustomFunctions(intrinsics::CustomFunction::FunctionMap\u0026 fn_map);` allows that. see [extensibility API documentation](api.md) to understand where it comes from.\n",
        "\n",
        "\n",
        "Under [`gemini_parser.h`](../cc/modules/parsers/gemini_parser.h)\n",
        "```c++\n",
        "// Parsers for Gemini model.\n",
        "class GeminiParser final {\n",
        " public:\n",
        "  ~GeminiParser() = default;\n",
        "\n",
        "  // Extract Top Candidate as Text.\n",
        "  static absl::StatusOr\u003cv0::Value\u003e GetTopCandidateAsText(v0::Value input);\n",
        "\n",
        "  // Wraps a text as Gemini request JSON.\n",
        "  static absl::StatusOr\u003cv0::Value\u003e WrapTextAsInputJson(v0::Value input);\n",
        "\n",
        "  // Make Parser functions visible to the runtime.\n",
        "  static absl::Status SetCustomFunctions(\n",
        "      intrinsics::CustomFunction::FunctionMap\u0026 fn_map);\n",
        "\n",
        "  // Not copyable or movable.\n",
        "  GeminiParser(const GeminiParser\u0026) = delete;\n",
        "  GeminiParser\u0026 operator=(const GeminiParser\u0026) = delete;\n",
        "\n",
        " private:\n",
        "  // Do not hold states in this class.\n",
        "  GeminiParser() = default;\n",
        "};\n",
        "```"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EQnD3UCd-sIT"
      },
      "source": [
        "### 1.3 Define the functions.\n",
        "\n",
        "Here we have an input formatter to Gemini and an output parser.\n",
        "\n",
        "Under [`gemini_parser.cc`](../cc/modules/parsers/gemini_parser.cc)\n",
        "``` c++\n",
        "absl::StatusOr\u003cv0::Value\u003e GeminiParser::GetTopCandidateAsText(v0::Value input) {\n",
        "  auto parsed_json = nlohmann::json::parse(input.str(), /*cb=*/nullptr,\n",
        "                                           /*allow_exceptions=*/false);\n",
        "  if (parsed_json.is_discarded()) {\n",
        "    return absl::InternalError(absl::Substitute(\n",
        "        \"Failed parsing json output from Gemini: $0\", input.DebugString()));\n",
        "  }\n",
        "\n",
        "  std::string extract_first_candidate_as_text =\n",
        "      \"{% if candidates %}{% for p in candidates.0.content.parts \"\n",
        "      \"%}{{p.text}}{% endfor %}{%   endif %}\";\n",
        "\n",
        "  inja_status_or::Environment env;\n",
        "\n",
        "  v0::Value result;\n",
        "  std::string result_str =\n",
        "      GENC_TRY(env.render(extract_first_candidate_as_text, parsed_json));\n",
        "  result.set_str(result_str);\n",
        "  return result;\n",
        "}\n",
        "\n",
        "absl::StatusOr\u003cv0::Value\u003e GeminiParser::WrapTextAsInputJson(v0::Value input) {\n",
        "  std::string json_request = absl::Substitute(\n",
        "      R\"pb(\n",
        "        {\n",
        "          \"contents\":\n",
        "          [ {\n",
        "            \"parts\":\n",
        "            [ { \"text\": \"$0\" }]\n",
        "          }]\n",
        "        }\n",
        "      )pb\",\n",
        "      input.str());\n",
        "  v0::Value result;\n",
        "  result.set_str(json_request);\n",
        "  return result;\n",
        "}\n",
        "```"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-qDqFddi_QEU"
      },
      "source": [
        "### 1.4 Make your functions visible to the runtime\n",
        "\n",
        "These are simple Lamda that wraps the newly defined functions. Notice that the routing uri such as `/gemini_parser/get_top_candidate_as_text` is totally up to your choice, it simply tells the runtime which function to invoke when it see an incoming compuation.\n",
        "\n",
        "Under `gemini_parser.cc`\n",
        "```c++\n",
        "absl::Status GeminiParser::SetCustomFunctions(\n",
        "    intrinsics::CustomFunction::FunctionMap\u0026 fn_map) {\n",
        "  fn_map[\"/gemini_parser/get_top_candidate_as_text\"] =\n",
        "      [](const v0::Value\u0026 arg) {\n",
        "        return GeminiParser::GetTopCandidateAsText(arg);\n",
        "      };\n",
        "\n",
        "  fn_map[\"/gemini_parser/wrap_text_as_input_json\"] = [](const v0::Value\u0026 arg) {\n",
        "    return GeminiParser::WrapTextAsInputJson(arg);\n",
        "  };\n",
        "\n",
        "  return absl::OkStatus();\n",
        "}\n",
        "```"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nVHtUBklAdeZ"
      },
      "source": [
        "### 1.5 Wire it into the runtime\n",
        "\n",
        "Under `executor_stack.cc`\n",
        "\n",
        "```c++\n",
        "GENC_TRY(GeminiParser::SetCustomFunctions(config.custom_function_map));\n",
        "```\n",
        "\n",
        "A word on executor stack:\n",
        "\n",
        "* `config` is a global context, you define it in your executor_stack.\n",
        "* the executor stacks we provide is for your conveninece, in practice you can create your own. The benefit is that each stack contians only the modules you need and nothing more.\n",
        "\n",
        "If you are interested read futher in Section 4: Executor Stacks.\n",
        "Also see [extensibility API documentation](api.md)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GEycBIiUAiNB"
      },
      "source": [
        "### 1.6 Try it out.\n",
        "\n",
        "Combine rest call and I/O parsers to form a custom model call chain.\n",
        "These are available in C++ also with similar syntax, please check our c++ examples.\n",
        "\n",
        "A python example can be located in [math_tool_agent.py](../python/examples/math_tool_agent.py)\n",
        "A c++ example can be located in [run_gemini_on_ai_studio.cc](../cc/examples/run_gemini_on_ai_studio.cc)\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "xQzDTG3FBaE1"
      },
      "outputs": [],
      "source": [
        "import generative_computing as genc\n",
        "\n",
        "rest_call = genc.authoring.create_rest_call(\"\u003cend point with api key\u003e\")\n",
        "str_to_json_request = genc.authoring.create_custom_function(\n",
        "    \"/gemini_parser/wrap_text_as_input_json\"\n",
        ")\n",
        "extrat_top_candidate = genc.authoring.create_custom_function(\n",
        "    \"/gemini_parser/get_top_candidate_as_text\"\n",
        ")\n",
        "\n",
        "model_call = (\n",
        "    genc.interop.langchain.CustomChain()\n",
        "    | str_to_json_request\n",
        "    | rest_call\n",
        "    | extrat_top_candidate\n",
        ")\n",
        "\n",
        "model_call_chain = genc.interop.langchain.create_computation(model_call)\n",
        "runner = genc.runtime.Runner(model_call_chain)\n",
        "runner(\"Tell me a story\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "f7ft3_qUZCBI"
      },
      "source": [
        "## Intrinsics\n",
        "You can consider them as native building blocks of your custom SDK.\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-m6PBZLwsKxx"
      },
      "source": [
        "## ModelInference\n",
        "GenC provides some predefined ways to call popular model backends such as ChatGPT, Gemini. The reality is that you'll wan to customize them for your own use cases.\n",
        "\n",
        "There are many model backends out there. Gemini, ChatGPT, self hosted LLAMA... each comes with a different input and output. Is there a way to bring all of them under a consistent interface, so you can easily chain them with other operators or switch between diffent models? This section provides the solution to this question."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yH0b5l4tZqAt"
      },
      "source": [
        "## ExecutorStack\n",
        "\n",
        "A custom runtime that holds context/state and brings all building blocks together."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8h0iaFaktREP"
      },
      "source": [
        "## Intro to next tutorial: Buidling Modular Agents\n",
        "\n",
        "Congratualtions you just made enough modular components to build a more interesting LLM agents. Next tutorial we'll see in action how all these building blocks will come together to create an LLM agent."
      ]
    }
  ],
  "metadata": {
    "colab": {
      "private_outputs": true,
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
